{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "q-5nsb6y9Gf7"
            },
            "source": [
                "# Import packages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import comet_ml\n",
                "comet_ml.init(project_name='seq-rec')\n",
                "from comet_ml import Experiment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Cuab_1B0mbjD"
            },
            "outputs": [],
            "source": [
                "from __future__ import absolute_import, division, print_function, unicode_literals\n",
                "\n",
                "import os\n",
                "from six.moves import urllib\n",
                "import tempfile\n",
                "\n",
                "from typing import Dict, Text\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import tensorflow as tf\n",
                "import tensorflow_recommenders as tfrs\n",
                "\n",
                "from google.cloud import bigquery\n",
                "from google.api_core.exceptions import GoogleAPIError\n",
                "from tensorflow.python.framework import ops\n",
                "from tensorflow.python.framework import dtypes\n",
                "from tensorflow_io.bigquery import BigQueryClient\n",
                "from tensorflow_io.bigquery import BigQueryReadSession"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "LDYnA-BwNOn-"
            },
            "outputs": [],
            "source": [
                "USE_GPU = True\n",
                "if USE_GPU:\n",
                "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
                "    device_name = tf.test.gpu_device_name()\n",
                "    if device_name != '/device:GPU:0':\n",
                "        raise SystemError('GPU device not found')\n",
                "    print('Found GPU at: {}'.format(device_name))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "TTqa04KH9KzF"
            },
            "source": [
                "# Configure auth to GCP resources"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "OyPDpLDIUv_d"
            },
            "outputs": [],
            "source": [
                "PROJECT_ID = \"seq-rec-gcp-project-id\"\n",
                "os.environ['PROJECT_ID'] = PROJECT_ID\n",
                "os.environ['GCLOUD_PROJECT'] = PROJECT_ID"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "JzelKzTuN-IE",
                "outputId": "b170e125-c76d-462f-d36e-9ff8dfd97e71"
            },
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "gcloud config set project ${PROJECT_ID}\n",
                "env GCLOUD_PROJECT=${PROJECT_ID}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "6icTulOW9PZ5"
            },
            "source": [
                "# Download input data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "iV4iIFAoRC8P"
            },
            "outputs": [],
            "source": [
                "LOCATION = 'us'\n",
                "\n",
                "DATASET_ID = 'data_science_dbt'\n",
                "TRAINING_TABLE_ID = 'fct_seq_rec_pad_train_30d'\n",
                "EVAL_TABLE_ID = 'fct_seq_rec_pad_eval_val'\n",
                "TEST_TABLE_ID = 'fct_seq_rec_pad_eval_test'\n",
                "\n",
                "CSV_SCHEMA = [\n",
                "      bigquery.SchemaField(\"time\", \"TIMESTAMP\"),\n",
                "      bigquery.SchemaField(\"user_id\", \"STRING\"),\n",
                "      bigquery.SchemaField(\"event_name\", \"STRING\"),\n",
                "      bigquery.SchemaField(\"merchant_id\", \"STRING\"),\n",
                "      bigquery.SchemaField(\"prev_search_term_list\", \"STRING\"),\n",
                "      bigquery.SchemaField(\"prev_event_ruid_list\", \"STRING\"),\n",
                "      bigquery.SchemaField(\"prev_search_term_time_diff_seconds_list\", \"STRING\"),\n",
                "      bigquery.SchemaField(\"prev_event_time_diff_seconds_list\", \"STRING\"),\n",
                "  ]\n",
                "\n",
                "UNUSED_COLUMNS = ['time']\n",
                "\n",
                "def transform_row(row_dict):\n",
                "    # Trim all string tensors\n",
                "    features_dict = { column:\n",
                "                    (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
                "                    for (column, tensor) in row_dict.items()\n",
                "                    }\n",
                "    return features_dict\n",
                "\n",
                "def read_bigquery(table_name):\n",
                "    tensorflow_io_bigquery_client = BigQueryClient()\n",
                "    read_session = tensorflow_io_bigquery_client.read_session(\n",
                "        \"projects/\" + PROJECT_ID,\n",
                "        PROJECT_ID, table_name, DATASET_ID,\n",
                "        list(field.name for field in CSV_SCHEMA \n",
                "              if not field.name in UNUSED_COLUMNS),\n",
                "        list(dtypes.double if field.field_type == 'FLOAT64' \n",
                "              else dtypes.string for field in CSV_SCHEMA\n",
                "              if not field.name in UNUSED_COLUMNS),\n",
                "        requested_streams=2)\n",
                "\n",
                "    dataset = read_session.parallel_read_rows()\n",
                "    transformed_ds = dataset.map(transform_row)\n",
                "    return transformed_ds\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "miFRXW56ydM8"
            },
            "outputs": [],
            "source": [
                "BATCH_SIZE = 32\n",
                "RANDOM_SEED = 13\n",
                "SHUFFLE_BUFFER_SIZE = 1_000_000\n",
                "\n",
                "tf.random.set_seed(RANDOM_SEED)\n",
                "\n",
                "training_ds = read_bigquery(TRAINING_TABLE_ID).shuffle(SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration=False)\n",
                "eval_ds = read_bigquery(EVAL_TABLE_ID)\n",
                "test_ds = read_bigquery(TEST_TABLE_ID)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "HBCIpEdAzNDK"
            },
            "outputs": [],
            "source": [
                "merchant_ids = training_ds.batch(1_000_000).map(lambda x: x[\"merchant_id\"])\n",
                "user_ids = training_ds.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
                "\n",
                "unique_merchant_ids = np.unique(np.concatenate(list(merchant_ids)))\n",
                "# unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "UmlQkKKG_cGk"
            },
            "outputs": [],
            "source": [
                "def calc_sample_weight(event_name: str):\n",
                "    if event_name == 'View_Merchant':\n",
                "        return 1\n",
                "    if event_name == 'Transaction_Success':\n",
                "        return 5\n",
                "    return 1\n",
                "\n",
                "def transform_train_data(ds):\n",
                "    return ds.map(lambda x: {\n",
                "        \"target_merchant_id\": x[\"merchant_id\"],\n",
                "        \"context_search_terms\": tf.strings.split(x['prev_search_term_list'], sep='|'),\n",
                "        \"context_search_terms_len\": len(tf.strings.split(x['prev_search_term_list'], sep='|')),\n",
                "        \"context_merchants\": tf.strings.split(x['prev_event_ruid_list'], sep='|'),\n",
                "        \"context_search_terms_time_recency\": tf.strings.split(x['prev_search_term_time_diff_seconds_list'], sep='|'),\n",
                "        \"context_merchants_time_recency\": tf.strings.split(x['prev_event_time_diff_seconds_list'], sep='|'),\n",
                "        \"user_id\": x[\"user_id\"],\n",
                "        \"sample_weight\": calc_sample_weight(x['event_name'])\n",
                "    })\n",
                "\n",
                "training_ds_prep = transform_train_data(training_ds)\n",
                "eval_ds_prep = transform_train_data(eval_ds)\n",
                "test_ds_prep = transform_train_data(test_ds)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "m8NDTcu7ferl"
            },
            "outputs": [],
            "source": [
                "search_terms = training_ds_prep.batch(1_000_000).map(lambda x: x['context_search_terms'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "PQcacRJ9AyvO",
                "outputId": "f2b320f3-24be-4576-a62f-68934adaca09"
            },
            "outputs": [],
            "source": [
                "for item in training_ds_prep.take(1):\n",
                "    print(item)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "PBjLnDkx89qp"
            },
            "source": [
                "# Modeling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "AOLuBYClPbNt"
            },
            "outputs": [],
            "source": [
                "embedding_dimension = 64\n",
                "max_search_term_tokens = 10000\n",
                "time_recency_num_buckets = 61 # Due to at query building we specify 1800s divide by 30s\n",
                "time_recency_buckets = np.array(list(map(str, range(time_recency_num_buckets))))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "XpMWqaao-crL"
            },
            "source": [
                "## The candidate tower"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Yw7iDiRY-iEq"
            },
            "outputs": [],
            "source": [
                "class CandidateModel(tf.keras.Model):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "\n",
                "        target_input = tf.keras.Input(shape=[None,], dtype=tf.string)\n",
                "        x = tf.keras.layers.StringLookup(vocabulary=unique_merchant_ids, mask_token=None)(target_input)\n",
                "        merchant_embedding_output = tf.keras.layers.Embedding(len(unique_merchant_ids) + 1, embedding_dimension)(x)\n",
                "        self.merchant_embedding = tf.keras.Model([target_input], merchant_embedding_output, name='target_embedding')\n",
                "\n",
                "    def call(self, merchant_ids):\n",
                "        return self.merchant_embedding(merchant_ids)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "orLi2v-8DDE_",
                "outputId": "3a23e240-1590-4163-e364-4b87ca2ea00d"
            },
            "outputs": [],
            "source": [
                "# Test output given an input\n",
                "candidate_model = CandidateModel()\n",
                "candidate_model(np.array([[b'<EXAMPLE_MERCHANT_ID>']]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "HuVLF0x_-3TE"
            },
            "source": [
                "## The query tower"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "_j40RyA0PwEr"
            },
            "outputs": [],
            "source": [
                "class QueryModel(tf.keras.Model):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "\n",
                "        # Context Merchants\n",
                "        context_merchants_inputs = tf.keras.Input(shape=[None,], dtype=tf.string)\n",
                "        x = tf.keras.layers.StringLookup(vocabulary=unique_merchant_ids, mask_token=None)(context_merchants_inputs)  # If specifying mask_token = 'NULL' then weird indices error occurs... Anyway we don't need to specify the mask_token since the NULL is left out already because of using fixed vocab\n",
                "        merchant_embedding = tf.keras.layers.Embedding(input_dim=len(unique_merchant_ids) + 1, output_dim=embedding_dimension)(x)\n",
                "        \n",
                "        context_merchants_time_recency_inputs = tf.keras.Input(shape=[None,], dtype=tf.string)\n",
                "        x = tf.keras.layers.StringLookup(vocabulary=time_recency_buckets, mask_token=None)(context_merchants_time_recency_inputs)\n",
                "        merchant_recency_embedding = tf.keras.layers.Embedding(input_dim=len(time_recency_buckets) + 1, output_dim=embedding_dimension)(x)\n",
                "        \n",
                "        merchant_features_embedding = tf.concat([merchant_embedding, merchant_recency_embedding], axis=2)\n",
                "        context_merchants_outputs = tf.keras.layers.GRU(embedding_dimension)(merchant_features_embedding)\n",
                "        self.context_merchants_embedding = tf.keras.Model([context_merchants_inputs, context_merchants_time_recency_inputs], context_merchants_outputs, name='context_merchants_embedding')\n",
                "\n",
                "        # Context Search Terms\n",
                "        context_search_terms_inputs = tf.keras.Input(shape=[None,], dtype=tf.string)\n",
                "        self.search_term_string_lookup_layer = tf.keras.layers.StringLookup(\n",
                "            max_tokens=max_search_term_tokens,\n",
                "            mask_token='NULL'\n",
                "        )\n",
                "        self.search_term_string_lookup_layer.adapt(search_terms)\n",
                "        x = self.search_term_string_lookup_layer(context_search_terms_inputs)\n",
                "        search_term_embedding = tf.keras.layers.Embedding(input_dim=self.search_term_string_lookup_layer.vocabulary_size(), output_dim=embedding_dimension)(x)\n",
                "\n",
                "        context_search_terms_time_recency_inputs = tf.keras.Input(shape=[None,], dtype=tf.string)\n",
                "        x = tf.keras.layers.StringLookup(vocabulary=time_recency_buckets, mask_token=None)(context_search_terms_time_recency_inputs)\n",
                "        search_term_recency_embedding = tf.keras.layers.Embedding(input_dim=len(time_recency_buckets) + 1, output_dim=embedding_dimension)(x)\n",
                "        \n",
                "        search_term_features_embedding = tf.concat([search_term_embedding, search_term_recency_embedding], axis=2)\n",
                "        context_search_terms_outputs = tf.keras.layers.GRU(embedding_dimension)(search_term_features_embedding)\n",
                "        self.context_search_terms_embedding = tf.keras.Model([context_search_terms_inputs, context_search_terms_time_recency_inputs], context_search_terms_outputs, name='context_search_terms_embedding')\n",
                "        \n",
                "        # Adding user_id introduces huge overfit. Need to know how to control this overfit before adding this.\n",
                "        # user_id_input = tf.keras.Input(shape=[None,], dtype=tf.string)\n",
                "        # x = tf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None)(user_id_input)\n",
                "        # user_id_output = tf.keras.layers.Embedding(input_dim=len(unique_user_ids) + 1, output_dim=embedding_dimension)(x)\n",
                "        # self.user_embedding = tf.keras.Model(user_id_input, user_id_output, name='user_id_embedding')\n",
                "\n",
                "    def call(self, inputs):\n",
                "        return tf.concat([\n",
                "            self.context_search_terms_embedding([inputs['context_search_terms'], inputs['context_search_terms_time_recency']]),\n",
                "            self.context_merchants_embedding([inputs['context_merchants'], inputs['context_merchants_time_recency']]),\n",
                "            # self.user_embedding(inputs['user_id'])\n",
                "        ], axis=1)"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "id": "I4AcyPRBXT0s"
            },
            "source": [
                "query_model = QueryModel()"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "xPz-ys7wRlgd",
                "outputId": "70ac7fb6-44cf-4755-a64a-e27e9c19ed0c"
            },
            "source": [
                "query_model.context_merchants_embedding.summary()"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "oozau4B2kfBD",
                "outputId": "4c6a8925-4e72-4ec1-b776-dab4a0f109f2"
            },
            "source": [
                "query_model.context_merchants_embedding.layers[1](np.array([[b'<EXAMPLE_MERCHANT_ID>']]))"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "QwGYo1ccXGTm",
                "outputId": "e680a545-b3fd-4093-ddbe-7cbdf7f7c88f"
            },
            "source": [
                "# Test output given an input\n",
                "query_model.context_merchants_embedding.layers[2](\n",
                "    query_model.context_merchants_embedding.layers[1](np.array([[b'<EXAMPLE_MERCHANT_ID>']]))\n",
                ")"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "jLjEDj0WSH4P",
                "outputId": "c5d5c5d3-418d-4b91-84d0-25f3be4f86a8"
            },
            "source": [
                "query_model.context_search_terms_embedding.summary()"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "xZNi2GFbjAGv",
                "outputId": "e430cc97-0785-4fdc-ea1e-3b190cc330e3"
            },
            "source": [
                "query_model.search_term_string_lookup_layer.vocabulary_size()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "b4ZnoCJajmcl",
                "tags": []
            },
            "source": [
                "# The full model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "qOHhsOBljyLR"
            },
            "outputs": [],
            "source": [
                "class SequentialRecModel(tfrs.models.Model):\n",
                "\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.query_model = tf.keras.Sequential([\n",
                "            QueryModel(),\n",
                "            tf.keras.layers.Dense(embedding_dimension),\n",
                "        ])\n",
                "        self.candidate_model = tf.keras.Sequential([\n",
                "            CandidateModel(),\n",
                "            tf.keras.layers.Dense(embedding_dimension),\n",
                "        ])\n",
                "        self.task = tfrs.tasks.Retrieval(\n",
                "            metrics=tfrs.metrics.FactorizedTopK(\n",
                "                candidates=tf.data.Dataset.from_tensor_slices(unique_merchant_ids).batch(128).map(self.candidate_model),\n",
                "            ),\n",
                "            # batch_metrics=[tf.keras.metrics.AUC]  # Can not use because missing y_pred error\n",
                "        )\n",
                "\n",
                "    def compute_loss(self, features, training=False):\n",
                "        query_embeddings = self.query_model({\n",
                "            \"context_search_terms\": features[\"context_search_terms\"],\n",
                "            \"context_merchants\": features[\"context_merchants\"],\n",
                "            \"context_merchants_time_recency\": features[\"context_merchants_time_recency\"],\n",
                "            \"context_search_terms_time_recency\": features[\"context_search_terms_time_recency\"],\n",
                "            # \"user_id\": features[\"user_id\"]\n",
                "        })\n",
                "        candidate_embeddings = self.candidate_model(features['target_merchant_id'])\n",
                "        sample_weight = features['sample_weight']\n",
                "\n",
                "        return self.task(query_embeddings, candidate_embeddings, sample_weight)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "epochs = 30\n",
                "optimizer = tf.keras.optimizers.Adam(0.003)\n",
                "optimizer_log_fmt = optimizer.get_config()\n",
                "batch_size = 256\n",
                "input_data_window = '30d'\n",
                "sample_weight = {\n",
                "    \"View_Merchant\": 1,\n",
                "    \"Transaction_Success\": 5\n",
                "}\n",
                "\n",
                "params = {\n",
                "    'batch_size': batch_size,\n",
                "    'epochs': epochs,\n",
                "    'optimizer': optimizer_log_fmt,\n",
                "    'embedding_dimension': embedding_dimension,\n",
                "    'max_search_term_tokens': max_search_term_tokens,\n",
                "    'input_data_window': input_data_window,\n",
                "    'sample_weight': sample_weight\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = SequentialRecModel()\n",
                "model.compile(optimizer=optimizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.query_model.layers[0].context_merchants_embedding.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.query_model.layers[0].context_search_terms_embedding.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "XsTx7rDNpQWV",
                "tags": []
            },
            "source": [
                "# Fitting and evaluating"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cached_train = training_ds_prep.shuffle(1_000_000).batch(batch_size).cache()\n",
                "cached_eval = eval_ds_prep.batch(batch_size).cache()\n",
                "cached_test = test_ds_prep.batch(batch_size).cache()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "## Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#create an experiment with your api key\n",
                "experiment = Experiment(\n",
                "    auto_metric_logging=True,\n",
                "    auto_param_logging=False,\n",
                "    auto_histogram_weight_logging=True,\n",
                "    auto_histogram_gradient_logging=True,\n",
                "    auto_histogram_activation_logging=True,\n",
                "    auto_histogram_tensorboard_logging=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "callback_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
                "    monitor='val_factorized_top_k/top_10_categorical_accuracy',\n",
                "    min_delta=0.001,\n",
                "    patience=3,\n",
                "    verbose=1,\n",
                "    mode='auto',\n",
                "    baseline=None,\n",
                "    restore_best_weights=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.fit(cached_train, epochs=epochs, callbacks=[callback_early_stopping], validation_data=cached_eval)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "metrics_test = model.evaluate(cached_test, return_dict=True)\n",
                "print(metrics_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "Hv441nrbpmwZ",
                "outputId": "15cd0980-4334-455c-b722-d1ff4ccbac4a"
            },
            "outputs": [],
            "source": [
                "%%time\n",
                "with experiment.train():\n",
                "    model.fit(cached_train, epochs=epochs, callbacks=[callback_early_stopping], validation_data=cached_eval)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "nVKkAe7YuBUO",
                "outputId": "7e4a190e-6910-4629-d3a3-887ba31d6b2d"
            },
            "outputs": [],
            "source": [
                "with experiment.test():\n",
                "    metrics_test = model.evaluate(cached_test, return_dict=True)\n",
                "    print(metrics_test)\n",
                "    experiment.log_metrics(metrics_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "experiment.log_parameters(params)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "experiment.end()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "# Save embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_embeddings(embedding_keys, embedding_values, name):\n",
                "    log_dir = f'logs/embeddings/{name}/'\n",
                "    if not os.path.exists(log_dir):\n",
                "        os.makedirs(log_dir)\n",
                "    else:\n",
                "        raise Exception(f'{log_dir} already exists')\n",
                "\n",
                "    with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
                "        for key in embedding_keys:\n",
                "            f.write(\"{}\\n\".format(key))\n",
                "    \n",
                "    weights = tf.Variable(embedding_values)\n",
                "    checkpoint = tf.train.Checkpoint(embedding=weights)\n",
                "    checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
                "    \n",
                "    return True"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {},
            "source": [
                "!rm -r ./logs/embeddings/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "search_term_weights = model.query_model.layers[0].context_search_terms_embedding.layers[4].get_weights()[0]\n",
                "search_term_keys = model.query_model.layers[0].context_search_terms_embedding.layers[2].get_vocabulary()\n",
                "save_embeddings(search_term_keys, search_term_weights, name='search_terms')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "merchant_weights = model.query_model.layers[0].context_merchants_embedding.layers[4].get_weights()[0]\n",
                "merchant_keys = model.query_model.layers[0].context_merchants_embedding.layers[2].get_vocabulary()\n",
                "save_embeddings(merchant_keys, merchant_weights, name='merchants')"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "tags": []
            },
            "source": [
                "# Set up a logs directory, so Tensorboard knows where to look for files.\n",
                "log_dir = 'logs/embeddings/search_terms/'\n",
                "if not os.path.exists(log_dir):\n",
                "    os.makedirs(log_dir)"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {},
            "source": [
                "# Save Labels separately on a line-by-line manner.\n",
                "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
                "    for term in search_term_key:\n",
                "        f.write(\"{}\\n\".format(term))"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {},
            "source": [
                "weights = tf.Variable(search_term_weights)\n",
                "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
                "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "e3Ju6P7qrVnR"
            },
            "source": [
                "# Test predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "65zX-YYsBKJh",
                "outputId": "a65c9346-300d-49f0-f4af-c8c0e072162a"
            },
            "outputs": [],
            "source": [
                "# TODO: Find how to deploy ScaNN to tensorflow serving\n",
                "# index = tfrs.layers.factorized_top_k.ScaNN(model.query_model)\n",
                "\n",
                "index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
                "index.index_from_dataset(\n",
                "  tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(unique_merchant_ids).batch(100), tf.data.Dataset.from_tensor_slices(unique_merchant_ids).batch(100).map(model.candidate_model)))\n",
                ")\n",
                "\n",
                "input = {\n",
                "    'context_merchants': np.array([[b'<EXAMPLE_MERCHANT_ID>']]),\n",
                "    'context_search_terms': np.array([[b'<EXAMPLE_SEARCH_TERM>']]),\n",
                "    \"context_merchants_time_recency\": np.array([[b'1']]),\n",
                "    \"context_search_terms_time_recency\": np.array([[b'1']])\n",
                "}\n",
                "\n",
                "_, recommendations = index(input)\n",
                "print(f\"Recommendations: {recommendations[0, :10]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "KConFZa7sU6o"
            },
            "source": [
                "# Export for serving"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_NAME = 'seq-rec-model-v0'\n",
                "SAVED_MODEL_PATH = f'models/{MODEL_NAME}'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "signature_dict = {\n",
                "    'context_merchants': tf.TensorSpec(shape=[None, 1], dtype=tf.string, name='context_merchants'),\n",
                "    'context_search_terms': tf.TensorSpec(shape=[None, 1], dtype=tf.string, name='context_search_terms'),\n",
                "    'context_merchants_time_recency': tf.TensorSpec(shape=[None, 1], dtype=tf.string, name='context_merchants_time_recency'),\n",
                "    'context_search_terms_time_recency': tf.TensorSpec(shape=[None, 1], dtype=tf.string, name='context_search_terms_time_recency'),\n",
                "}\n",
                "\n",
                "@tf.function(input_signature=[signature_dict])\n",
                "def rec_at_10(data):\n",
                "    result = index(data, k=10)\n",
                "    return {\n",
                "        \"scores\": result[0],\n",
                "        \"merchant_id\": result[1]\n",
                "    }\n",
                "\n",
                "@tf.function(input_signature=[signature_dict])\n",
                "def rec_at_100(data):\n",
                "    result = index(data, k=100)\n",
                "    return {\n",
                "        \"scores\": result[0],\n",
                "        \"merchant_id\": result[1]\n",
                "    }\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rec_at_100(input)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tf.saved_model.save(\n",
                "    index,\n",
                "    SAVED_MODEL_PATH,\n",
                "    signatures={\n",
                "        \"serving_default\": rec_at_100,\n",
                "        \"k_10\": rec_at_10,\n",
                "        \"k_100\": rec_at_100,\n",
                "    }\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "IHPaV1nN9MO2",
                "outputId": "91adfcf2-fafe-40c4-d13e-e1df66fa42fc",
                "tags": []
            },
            "outputs": [],
            "source": [
                "loaded = tf.saved_model.load(SAVED_MODEL_PATH)\n",
                "scores, titles = loaded(input)\n",
                "print(f\"Recommendations: {titles[0][:10]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "loaded.signatures['serving_default'](**input)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "loaded.signatures['k_100'](**input)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "r5GJpbIp4_yp"
            },
            "source": [
                "## Analyze the signature to call the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Bjw4QOU65MG3"
            },
            "outputs": [],
            "source": [
                "os.environ['MODEL_EXPORT_PATH'] = SAVED_MODEL_PATH"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "Mjk6f1kP5DUX",
                "outputId": "bbbf9885-d36a-42a1-b357-f5b11be87e6d"
            },
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "saved_model_cli show --dir ${MODEL_EXPORT_PATH} \\\n",
                "     --tag_set serve --signature_def serving_default"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "lmE1V4_4uiBm"
            },
            "source": [
                "# Upload model to GCS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "h8DsLK_ts576"
            },
            "outputs": [],
            "source": [
                "import glob\n",
                "from google.cloud import storage\n",
                "\n",
                "def upload_local_directory_to_gcs(local_path, bucket_name, gcs_path):\n",
                "    gcs_client = storage.Client()\n",
                "\n",
                "    bucket = gcs_client.get_bucket(bucket_name)\n",
                "    assert os.path.isdir(local_path)\n",
                "    for local_file in glob.glob(local_path + '/**'):\n",
                "        if not os.path.isfile(local_file):\n",
                "            upload_local_directory_to_gcs(local_file, bucket, gcs_path + \"/\" + os.path.basename(local_file))\n",
                "        else:\n",
                "            remote_path = os.path.join(gcs_path, local_file[1 + len(local_path):])\n",
                "            blob = bucket.blob(remote_path)\n",
                "            blob.upload_from_filename(local_file)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "sWPf5llNvhAn"
            },
            "outputs": [],
            "source": [
                "BUCKET_LOCATION = \"ASIA-SOUTHEAST1\"\n",
                "BUCKET_NAME = \"recsys-pipeline\"\n",
                "BUCKET_FOLDER_DIR = F\"seq-rec/{MODEL_NAME}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "vzHNIdf6Ofqg"
            },
            "outputs": [],
            "source": [
                "upload_local_directory_to_gcs(SAVED_MODEL_PATH, BUCKET_NAME , BUCKET_FOLDER_DIR)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "taUce15UwA61"
            },
            "source": [
                "# Create endpoints"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "OlUyf_718ibT"
            },
            "source": [
                "Ref: https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api#aiplatform_create_endpoint_sample-gcloud"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "pPMzj0Xq7hP4"
            },
            "outputs": [],
            "source": [
                "ENDPOINT_VARS = dict(\n",
                "    ENDPOINT_LOCATION=\"asia-southeast1\",\n",
                "    ENDPOINT_NAME=\"seq-rec-model\",\n",
                "    ENDPOINT_VERSION=\"v0\",\n",
                "    MODEL_NAME=MODEL_NAME,\n",
                "    PATH_TO_MODEL_ARTIFACT_DIRECTORY=f\"gs://{BUCKET_NAME}/{BUCKET_FOLDER_DIR}\",\n",
                "    CONTAINER_IMAGE_URI=\"asia-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-7:latest\",\n",
                "    # CONTAINER_IMAGE_URI=\"asia.gcr.io/seq-rec-gcp-project-id/tf-serving-scann\",\n",
                "    ENDPOINT_MACHINE_TYPE=\"n1-standard-2\",\n",
                "    ENDPOINT_MIN_REPLICA_COUNT=\"1\",\n",
                "    ENDPOINT_MAX_REPLICA_COUNT=\"1\",\n",
                "    BUCKET_LOCATION=BUCKET_LOCATION,\n",
                "    BUCKET_NAME=BUCKET_NAME,\n",
                "    BUCKET_FOLDER_DIR=BUCKET_FOLDER_DIR\n",
                ")\n",
                "\n",
                "for var_key, var_value in ENDPOINT_VARS.items():\n",
                "    os.environ[var_key] = var_value"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8hWzmdB-NoYQ"
            },
            "source": [
                "## Upload model to Vertex AI Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import google.cloud.aiplatform as aip"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "aip.init(project=PROJECT_ID, location=ENDPOINT_VARS['ENDPOINT_LOCATION'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ref: https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/explainable_ai/sdk_custom_tabular_regression_online_explain.ipynb\n",
                "\n",
                "model = aip.Model.upload(\n",
                "    display_name=ENDPOINT_VARS['MODEL_NAME'],\n",
                "    artifact_uri=ENDPOINT_VARS['PATH_TO_MODEL_ARTIFACT_DIRECTORY'],\n",
                "    serving_container_image_uri=ENDPOINT_VARS['CONTAINER_IMAGE_URI'],\n",
                "    sync=False\n",
                ")\n",
                "\n",
                "model.wait()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "O6-2X8c281U1"
            },
            "source": [
                "### Deploy the model to endpoint"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "TRAFFIC_SPLIT = {\"0\": 100}\n",
                "DEPLOY_GPU = False\n",
                "\n",
                "endpoint = model.deploy(\n",
                "    deployed_model_display_name=ENDPOINT_VARS['MODEL_NAME'],\n",
                "    traffic_split=TRAFFIC_SPLIT,\n",
                "    machine_type=ENDPOINT_VARS['ENDPOINT_MACHINE_TYPE'],\n",
                "    accelerator_type=DEPLOY_GPU,\n",
                "    accelerator_count=0,\n",
                "    min_replica_count=int(ENDPOINT_VARS['ENDPOINT_MIN_REPLICA_COUNT']),\n",
                "    max_replica_count=int(ENDPOINT_VARS['ENDPOINT_MAX_REPLICA_COUNT']),\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ZKIzWyvsQGEj",
                "tags": []
            },
            "source": [
                "# Test the deployed model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "5mlKxwoTe8v0",
                "outputId": "43cb15c2-98a1-40be-b007-2eacd4e27cb2"
            },
            "outputs": [],
            "source": [
                "%%time\n",
                "instances = [\n",
                "    {\n",
                "        \"context_merchants\": [\"<EXAMPLE_MERCHANT_ID>\"],\n",
                "        \"context_search_terms\": [\"<EXAMPLE_SEARCH_TERM>\"],\n",
                "        \"context_merchants_time_recency\": [\"1\"],\n",
                "        \"context_search_terms_time_recency\": [\"1\"]\n",
                "    }\n",
                "]\n",
                "prediction = endpoint.predict(instances=instances)\n",
                "prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prediction.predictions[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "lyySWCS_woJi",
                "tags": []
            },
            "source": [
                "# Batch predictions with Vertex AI Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Union, Sequence\n",
                "from google.cloud import aiplatform, aiplatform_v1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "RqO5RVGvwqfP"
            },
            "outputs": [],
            "source": [
                "def create_batch_prediction_job_dedicated_resources(\n",
                "    project: str,\n",
                "    location: str,\n",
                "    model_resource_name: str,\n",
                "    job_display_name: str,\n",
                "    gcs_source: Union[str, Sequence[str]],\n",
                "    gcs_destination: str,\n",
                "    machine_type: str = \"n1-standard-2\",\n",
                "    accelerator_count: int = 1,\n",
                "    accelerator_type: Union[str, aiplatform_v1.AcceleratorType] = \"NVIDIA_TESLA_K80\",\n",
                "    starting_replica_count: int = 1,\n",
                "    max_replica_count: int = 1,\n",
                "    sync: bool = True,\n",
                "):\n",
                "    aiplatform.init(project=project, location=location)\n",
                "\n",
                "    my_model = aiplatform.Model(model_resource_name)\n",
                "\n",
                "    batch_prediction_job = my_model.batch_predict(\n",
                "        job_display_name=job_display_name,\n",
                "        gcs_source=gcs_source,\n",
                "        gcs_destination_prefix=gcs_destination,\n",
                "        machine_type=machine_type,\n",
                "        accelerator_count=accelerator_count,\n",
                "        accelerator_type=accelerator_type,\n",
                "        starting_replica_count=starting_replica_count,\n",
                "        max_replica_count=max_replica_count,\n",
                "        sync=sync,\n",
                "    )\n",
                "\n",
                "    batch_prediction_job.wait()\n",
                "\n",
                "    print(batch_prediction_job.display_name)\n",
                "    print(batch_prediction_job.resource_name)\n",
                "    print(batch_prediction_job.state)\n",
                "    return batch_prediction_job"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "sKOVjlj9xhfr"
            },
            "source": [
                "## Create source file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "o12nY_Wpxjo0",
                "outputId": "ac58d3f6-df72-4597-8606-2647acff92c0"
            },
            "outputs": [],
            "source": [
                "%%writefile sample_request_batch.jsonl\n",
                "{\"user_id\": \"<EXAMPLE_USER_ID>\", \"merchant_id\": \"<EXAMPLE_MERCHANT_ID_1>\"}\n",
                "{\"user_id\": \"<EXAMPLE_USER_ID>\", \"merchant_id\": \"<EXAMPLE_MERCHANT_ID_2>\"}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "gkI-alkXyGWe",
                "outputId": "f7edb231-72c8-4007-87e6-528cfa27aae5"
            },
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "gsutil cp sample_request_batch.jsonl gs://${BUCKET_NAME}/${BUCKET_FOLDER_DIR}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "LK8BBcPXxgRF",
                "outputId": "aa62e27c-0dd6-4790-fddf-30089f82777e"
            },
            "outputs": [],
            "source": [
                "create_batch_prediction_job_dedicated_resources(\n",
                "    project=PROJECT_ID,\n",
                "    location=ENDPOINT_VARS['ENDPOINT_LOCATION'],\n",
                "    model_resource_name=os.environ['MODEL_ID'],\n",
                "    job_display_name=\"test-batch-predict-user-ranking\",\n",
                "    gcs_source=f\"gs://{BUCKET_NAME}/{BUCKET_FOLDER_DIR}/sample_request_batch.jsonl\",\n",
                "    gcs_destination=f\"gs://{BUCKET_NAME}/{BUCKET_FOLDER_DIR}\", \n",
                "    machine_type=\"n1-standard-8\",\n",
                "    accelerator_count=0,\n",
                "    accelerator_type=None,\n",
                "    starting_replica_count=1,\n",
                "    max_replica_count=1,\n",
                "    sync=True\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "iEDbQUSpyCzR"
            },
            "source": [
                "### Delete job on the go"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "rbuOPIZG1cYF"
            },
            "outputs": [],
            "source": [
                "def cancel_batch_prediction_job(\n",
                "    project: str,\n",
                "    batch_prediction_job_id: str,\n",
                "    location: str = \"us-central1\",\n",
                "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
                "):\n",
                "    \"\"\" Source: https://cloud.google.com/vertex-ai/docs/samples/aiplatform-cancel-batch-prediction-job-sample#aiplatform_cancel_batch_prediction_job_sample-python\n",
                "    \"\"\"\n",
                "    # The AI Platform services require regional API endpoints.\n",
                "    client_options = {\"api_endpoint\": api_endpoint}\n",
                "    # Initialize client that will be used to create and send requests.\n",
                "    # This client only needs to be created once, and can be reused for multiple requests.\n",
                "    client = aiplatform.gapic.JobServiceClient(client_options=client_options)\n",
                "    name = client.batch_prediction_job_path(\n",
                "        project=project, location=location, batch_prediction_job=batch_prediction_job_id\n",
                "    )\n",
                "    response = client.cancel_batch_prediction_job(name=name)\n",
                "    print(\"response:\", response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "bxJzcbDD1dPr",
                "outputId": "8af0d348-4792-4a0b-f941-961a9b8f43ec"
            },
            "outputs": [],
            "source": [
                "cancel_batch_prediction_job(\n",
                "    project=PROJECT_ID,\n",
                "    batch_prediction_job_id=\"<EXAMPLE_PREDICTION_JOB_ID>\",\n",
                "    location=ENDPOINT_VARS['ENDPOINT_LOCATION'],\n",
                "    api_endpoint=f\"{ENDPOINT_VARS['ENDPOINT_LOCATION']}-aiplatform.googleapis.com\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MPpH5Is67nfX"
            },
            "source": [
                "## Undeploy a model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "k_qMDPIK7pHh"
            },
            "outputs": [],
            "source": [
                "def undeploy_model_in_endpoint(\n",
                "    end_point: str,\n",
                "    project: str,\n",
                "    model_id: str,\n",
                "    location: str = \"us-central1\",\n",
                "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
                "    timeout: int = 7200,\n",
                "):\n",
                "    # The AI Platform services require regional API endpoints.\n",
                "    client_options = {\"api_endpoint\": api_endpoint}\n",
                "    # Initialize client that will be used to create and send requests.\n",
                "    # This client only needs to be created once, and can be reused for multiple requests.\n",
                "    client = aiplatform.gapic.EndpointServiceClient(client_options=client_options)\n",
                "    client_model = aiplatform_v1.services.model_service.ModelServiceClient(client_options=client_options)\n",
                "\n",
                "    # Get deployed_model_id\n",
                "    model_name = f'projects/{project}/locations/{location}/models/{model_id}'\n",
                "    model_request = aiplatform_v1.types.GetModelRequest(name=model_name)\n",
                "    model_info = client_model.get_model(request=model_request)\n",
                "    deployed_models_info = model_info.deployed_models\n",
                "    deployed_model_id=model_info.deployed_models[0].deployed_model_id\n",
                "\n",
                "    name=f'projects/{project}/locations/{location}/endpoints/{end_point}'\n",
                "\n",
                "    undeploy_request = aiplatform_v1.types.UndeployModelRequest(endpoint=name,deployed_model_id=deployed_model_id)\n",
                "    client.undeploy_model(request=undeploy_request)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ['MODEL_ID']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "76byZHsd7v5e"
            },
            "outputs": [],
            "source": [
                "undeploy_model_in_endpoint(\n",
                "    end_point=os.environ['ENDPOINT_ID'],\n",
                "    project=PROJECT_ID,\n",
                "    model_id=\"<EXAMPLE_MODEL_ID>\",\n",
                "    location=os.environ['ENDPOINT_LOCATION'],\n",
                "    api_endpoint=f\"{ENDPOINT_VARS['ENDPOINT_LOCATION']}-aiplatform.googleapis.com\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "yVwEyRX2hOZw"
            },
            "source": [
                "## Delete a model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "hRh1OkEMhQzR"
            },
            "outputs": [],
            "source": [
                "# In some cases the model is orphan from the endpoint but somehow can not be deleted on the UI\n",
                "\n",
                "aiplatform.init(project=PROJECT_ID, location=os.environ['ENDPOINT_LOCATION'])\n",
                "my_model = aiplatform.Model(\"<EXAMPLE_MODEL_ID>\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "5YC3Mf3uhSY6"
            },
            "outputs": [],
            "source": [
                "my_model.delete(sync=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Jbl6dTBdp72p"
            },
            "source": [
                "# Archive"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "id": "JZySMh8Op80g"
            },
            "source": [
                "# class QueryModel(tf.keras.Model):\n",
                "#     def __init__(self):\n",
                "#         super().__init__()\n",
                "\n",
                "#         self.context_merchants_embedding = tf.keras.Sequential([\n",
                "#             tf.keras.layers.StringLookup(vocabulary=unique_merchant_ids, mask_token=None),  # If specifying mask_token = 'NULL' then weird indices error occurs... Anyway we don't need to specify the mask_token since the NULL is left out already because of using fixed vocab\n",
                "#             tf.keras.layers.Embedding(input_dim=len(unique_merchant_ids) + 1, output_dim=embedding_dimension),\n",
                "#             tf.keras.layers.LSTM(embedding_dimension)\n",
                "#         ])\n",
                "\n",
                "#         self.search_term_string_lookup_layer = tf.keras.layers.StringLookup(max_tokens=max_search_term_tokens, mask_token='NULL')\n",
                "#         self.search_term_string_lookup_layer.adapt(search_terms)\n",
                "#         self.context_search_terms_embedding = tf.keras.Sequential([\n",
                "#             tf.keras.layers.InputLayer(input_shape=[None,], dtype=tf.string),  # Have to include the input layer here because of this weird issue: https://github.com/keras-team/keras/issues/16101\n",
                "#             self.search_term_string_lookup_layer,\n",
                "#             tf.keras.layers.Embedding(input_dim=self.search_term_string_lookup_layer.vocabulary_size(), output_dim=embedding_dimension),\n",
                "#             tf.keras.layers.GlobalAveragePooling1D()\n",
                "#         ])\n",
                "        \n",
                "#         # self.context_search_terms_embedding = tf.keras.Sequential([\n",
                "#         #     tf.keras.layers.TextVectorization(max_tokens=max_search_term_tokens, split='|'),\n",
                "#         #     tf.keras.layers.Embedding(max_search_term_tokens, embedding_dimension, mask_zero=True),\n",
                "#         #     # We average the embedding of individual words to get one embedding vector\n",
                "#         #     # per title.\n",
                "#         #     tf.keras.layers.GlobalAveragePooling1D(),\n",
                "#         # ])\n",
                "\n",
                "#     def call(self, inputs):\n",
                "#         # return self.context_merchants_embedding(inputs['context_merchants'])\n",
                "#         return tf.concat([\n",
                "#             self.context_merchants_embedding(inputs['context_merchants']),\n",
                "#             self.context_search_terms_embedding(inputs['context_search_terms']),\n",
                "#         ], axis=1)"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {},
            "source": [
                "# if USE_GPU:\n",
                "#     physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
                "#     tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
                "#     tf.config.experimental.set_visible_devices(physical_devices[0], device_type='GPU')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Nxz0P5i-8gN7"
            },
            "source": [
                "## Create and endpoint"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "WhvEuMxC7cuX",
                "outputId": "dc34d03f-633c-4c7e-fcd6-c2a6b2ccaea9"
            },
            "source": [
                "%%bash\n",
                "\n",
                "gcloud ai endpoints create \\\n",
                "  --region=${ENDPOINT_LOCATION} \\\n",
                "  --display-name=${ENDPOINT_NAME}"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "xBAeVwbs8dIR",
                "outputId": "7f373dd2-4007-427c-fd9e-0d736a0bd6e3"
            },
            "source": [
                "%%bash\n",
                "\n",
                "gcloud ai endpoints list \\\n",
                "  --region=${ENDPOINT_LOCATION} \\\n",
                "  --filter=display_name=${ENDPOINT_NAME}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Upload model"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "L5Pw50HkNsx5",
                "outputId": "25b772b0-79da-4862-fa9a-7a9a39239454"
            },
            "source": [
                "%%bash\n",
                "\n",
                "gcloud ai models upload \\\n",
                "  --region=${ENDPOINT_LOCATION} \\\n",
                "  --display-name=${MODEL_NAME} \\\n",
                "  --container-image-uri=${CONTAINER_IMAGE_URI} \\\n",
                "  --project=${PROJECT_ID} \\\n",
                "  --artifact-uri=${PATH_TO_MODEL_ARTIFACT_DIRECTORY}"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "789nW4QQ3AmU",
                "outputId": "0f79ad95-1a12-47b5-c76b-557d51951223"
            },
            "source": [
                "%%bash\n",
                "\n",
                "gcloud ai models list \\\n",
                "  --region=${ENDPOINT_LOCATION} \\\n",
                "  --filter=display_name=${MODEL_NAME} \\"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "## Deploy model to endpoint"
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "id": "QSetTFkAPLrf"
            },
            "source": [
                "os.environ['MODEL_ID'] = \"<EXAMPLE_MODEL_ID>\"\n",
                "os.environ['ENDPOINT_ID'] = \"<EXAMPLE_ENDPOINT_ID>\""
            ]
        },
        {
            "cell_type": "raw",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "4BYLm22i86G1",
                "outputId": "c99bbdae-796b-44c9-a7fe-dc320f6ad623"
            },
            "source": [
                "%%bash\n",
                "\n",
                "gcloud ai endpoints deploy-model ${ENDPOINT_ID} \\\n",
                "  --region=${ENDPOINT_LOCATION} \\\n",
                "  --model=${MODEL_ID} \\\n",
                "  --display-name=${MODEL_NAME} \\\n",
                "  --machine-type=${ENDPOINT_MACHINE_TYPE} \\\n",
                "  --min-replica-count=${ENDPOINT_MIN_REPLICA_COUNT} \\\n",
                "  --max-replica-count=${ENDPOINT_MAX_REPLICA_COUNT} \\\n",
                "  --traffic-split=0=100"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "collapsed_sections": [],
            "name": "seq-rec-model-v0.ipynb",
            "provenance": []
        },
        "gpuClass": "standard",
        "kernelspec": {
            "display_name": "Python 3.8.14 ('.venv': poetry)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.14"
        },
        "vscode": {
            "interpreter": {
                "hash": "eaeefced7af4788b9b4a203895b09193a6dd449d207d1b1237ec1e0a47ffeed0"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
