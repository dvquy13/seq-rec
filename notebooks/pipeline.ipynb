{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "7aada0f9-1131-4486-8b24-f87cdd2d83bd",
            "metadata": {},
            "source": [
                "# Global Settings and Imports"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f17c9db9",
            "metadata": {},
            "source": [
                "## Import packages"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "56c191b7",
            "metadata": {},
            "outputs": [],
            "source": [
                "%reload_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "132ce6f4",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "from datetime import datetime\n",
                "\n",
                "import seq_rec.utils.custom_logging"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f93c962a-d321-4fdd-ad9e-1410e154d5a2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# This is for variable interpolation from notebook to writefile command (used when write the cloud function definition)\n",
                "from IPython.core.magic import register_line_cell_magic\n",
                "\n",
                "@register_line_cell_magic\n",
                "def writetemplate(line, cell):\n",
                "    with open(line, 'w') as f:\n",
                "        f.write(cell.format(**globals()))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ddde652f-f3e4-4d92-9b8a-320ff3115b10",
            "metadata": {},
            "outputs": [],
            "source": [
                "import google.cloud.aiplatform as aip\n",
                "from kfp.v2 import dsl\n",
                "import kfp\n",
                "from kfp.v2.dsl import Artifact, Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2b4be71d-e645-486b-8d27-6c6396b29cb7",
            "metadata": {},
            "outputs": [],
            "source": [
                "import seq_rec.utils as utils"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a007ac28",
            "metadata": {},
            "outputs": [],
            "source": [
                "from seq_rec.op.alert import slack_noti_exit_op, slack_noti_op, record_job_status_op, record_last_checkpoint_date_op\n",
                "from seq_rec.op.commons import copy_output_to_gcs_op\n",
                "from seq_rec.op.ingest import detect_resume_training_op, tf_download_bq_table_op\n",
                "from seq_rec.op.data_validation import generate_tfdv_schema_op, validate_tfdv_schema_op\n",
                "from seq_rec.op.preprocess import preprocess_op\n",
                "from seq_rec.op.model import model_op\n",
                "from seq_rec.op.evaluate import evaluate_op\n",
                "from seq_rec.op.deploy import deploy_model_to_gcp_endpoint_op, update_user_recent_txn_in_recommend_api_op"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e36a44e2-a893-4a67-b0c4-06eca9940e87",
            "metadata": {
                "tags": []
            },
            "source": [
                "## Parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "236c3ea1-2070-49eb-9f06-443715704428",
            "metadata": {},
            "outputs": [],
            "source": [
                "HYDRA_CONFIG_PATH = '../seq_rec/conf/'\n",
                "COUNTRY_CODE = 'SG'\n",
                "cfg = utils.load_cfg(HYDRA_CONFIG_PATH)\n",
                "\n",
                "# Google Cloud\n",
                "PROJECT_ID = cfg.env.gcp.project_id\n",
                "REGION = cfg.env.pipeline.kubeflow.region\n",
                "BUCKET_NAME = cfg.env.pipeline.kubeflow.bucket_name\n",
                "BUCKET_URL = cfg.env.pipeline.kubeflow.bucket_url\n",
                "BUCKET_DIR = cfg.env.pipeline.kubeflow.bucket_dir\n",
                "\n",
                "SERVICE_ACCOUNT_EMAIL = cfg.env.gcp.service_account_email\n",
                "\n",
                "VERSION = \"0.2.0\"\n",
                "VERSION_NODOT = VERSION.replace(\".\", \"\")\n",
                "PIPELINE_NAME = f\"seq_rec\"\n",
                "PIPELINE_ROOT_PREFIX = f\"{BUCKET_URL}/pipeline_root/{PIPELINE_NAME}/{VERSION_NODOT}\"\n",
                "\n",
                "JOB_STATUS_FILE_NAME = \"job_status.txt\"\n",
                "\n",
                "PIPELINE_STAKEHOLDERS_SLACK_UIDS = {\n",
                "    \"Quy MLE\": \"<EXAMPLE_SLACK_UID>\"  # TODO: Retrieve from secrets manager\n",
                "}\n",
                "\n",
                "# Model params\n",
                "RANDOM_SEED = 13  # Set None for non-deterministic result"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bd418491",
            "metadata": {},
            "source": [
                "# Define Pipeline"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9334c44d-1d51-4eb4-8ec1-d1db7dc905de",
            "metadata": {},
            "source": [
                "## Full Retraining"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a2e668bf",
            "metadata": {},
            "outputs": [],
            "source": [
                "@dsl.pipeline(\n",
                "    # Default pipeline root. You can override it when submitting the pipeline.\n",
                "    pipeline_root=PIPELINE_ROOT_PREFIX,\n",
                "    # A name for the pipeline. Use to determine the pipeline Context.\n",
                "    name=f\"{PIPELINE_NAME}{COUNTRY_CODE.lower()}fullretraining\",\n",
                ")\n",
                "def pipeline_full_retraining(\n",
                "        deployment_threshold: dict,\n",
                "        resume_training: bool = False\n",
                "    ):\n",
                "    \"\"\" Define the pipeline components for full retraining\n",
                "\n",
                "    Args:\n",
                "        deployment_threshold (dict): dictionary of metrics and thresholds to deploy\n",
                "        resume_training (bool): whether to pick up the model state from previous train and continue training\n",
                "    \"\"\"\n",
                "    run_id = dsl.PIPELINE_JOB_ID_PLACEHOLDER\n",
                "    run_name = dsl.PIPELINE_JOB_NAME_PLACEHOLDER\n",
                "    resource_name = dsl.PIPELINE_JOB_RESOURCE_NAME_PLACEHOLDER\n",
                "    run_status = '{{$.pipeline_job_status}} - {{$.pipeline_status}} - {{$.pipeline_worlflow_status}}'\n",
                "    url = f\"https://console.cloud.google.com/vertex-ai/locations/{REGION}/pipelines/runs/{run_name}?project={PROJECT_ID}\"\n",
                "\n",
                "    country_code = COUNTRY_CODE\n",
                "    model_blob_name = f\"{BUCKET_DIR}/{VERSION}/{country_code}/model/\"\n",
                "    checkpoint_blob_name = f\"{BUCKET_DIR}/{VERSION}/{country_code}/checkpoint/\"\n",
                "    pipeline_root_full = os.path.join(PIPELINE_ROOT_PREFIX, country_code, \"full_retraining\")\n",
                "    last_checkpoint_date_blob_name = os.path.join(checkpoint_blob_name, \"last_checkpoint_date.txt\")\n",
                "    ENV = 'prod'\n",
                "    \n",
                "    exit_text = (\n",
                "        f\"*Run name:* <{url}|{run_name}>\",\n",
                "        f\"*Run ID:* {run_id}\",\n",
                "    )\n",
                "    exit_text = '\\n'.join(exit_text)\n",
                "    message = [\n",
                "        {\n",
                "            \"type\": \"header\",\n",
                "            \"text\": {\n",
                "                \"type\": \"plain_text\",\n",
                "                \"text\": \"Kubeflow pipeline has completed!\",\n",
                "                \"emoji\": True\n",
                "            }\n",
                "        },\n",
                "        {\n",
                "            \"type\": \"divider\"\n",
                "        },\n",
                "        {\n",
                "            \"type\": \"section\",\n",
                "            \"text\": {\n",
                "                \"type\": \"mrkdwn\",\n",
                "                \"text\": exit_text\n",
                "            }\n",
                "        }\n",
                "    ]\n",
                "    \n",
                "    slack_noti_exit_task = slack_noti_exit_op(\n",
                "        webhook_url=os.environ.get('SLACK_INCOMING_WEBHOOK'),  # Please declare the SLACK_INCOMING_WEBHOOK as a var in .env file\n",
                "        message=message,\n",
                "        run_name=run_name,\n",
                "        job_status_file_name=JOB_STATUS_FILE_NAME,\n",
                "        bucket_url=BUCKET_URL,\n",
                "        folder=BUCKET_DIR,\n",
                "        pipeline_stakeholders_slack_uids=PIPELINE_STAKEHOLDERS_SLACK_UIDS\n",
                "    ).set_display_name(\"Exit Handler Slack Noti\")\n",
                "    \n",
                "    # Could not get the job status via pipeline utils so create a small hack here\n",
                "    # The below task should be triggered after the final tasks has completed\n",
                "    record_job_status_task = (\n",
                "        record_job_status_op(\n",
                "            run_name=run_name,\n",
                "            job_status_file_name=JOB_STATUS_FILE_NAME,\n",
                "            bucket_name=BUCKET_NAME,\n",
                "            folder=BUCKET_DIR,\n",
                "        )\n",
                "        .set_display_name(\"Record Job Status\")\n",
                "    )\n",
                "    # Since the input of this task does not change as we overwrite the same file, we need to disable caching for this task.\n",
                "    # record_job_status_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
                "    record_job_status_task.set_caching_options(False)\n",
                "\n",
                "    with dsl.ExitHandler(slack_noti_exit_task):\n",
                "        detect_resume_training_task = (\n",
                "            detect_resume_training_op(\n",
                "                resume_training=resume_training,\n",
                "                checkpoint_bucket=BUCKET_NAME,\n",
                "                last_checkpoint_date_blob_name=last_checkpoint_date_blob_name\n",
                "            )\n",
                "            .set_display_name(\"Detect and Prepare to Resume Training\")\n",
                "            .set_caching_options(False)\n",
                "        )\n",
                "\n",
                "        bq_download_train_test_task = (\n",
                "            tf_download_bq_table_op(\n",
                "                resume_training=detect_resume_training_task.outputs['resume_training'],\n",
                "                last_checkpoint_date=detect_resume_training_task.outputs['last_checkpoint_date'],\n",
                "                country_code=country_code\n",
                "            )\n",
                "            .set_cpu_limit(\"4\")\n",
                "            .set_memory_limit(\"16G\")\n",
                "            .add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
                "            .set_gpu_limit(\"1\")\n",
                "            .set_display_name(\"Download Train and Test Data for Evaluation\")\n",
                "            .set_caching_options(False)\n",
                "        )\n",
                "\n",
                "        gen_schema_task = (\n",
                "            generate_tfdv_schema_op(\n",
                "                x_train_file=bq_download_train_test_task.outputs['raw_train_ds_file']\n",
                "            )\n",
                "            .set_display_name(\"Generate Schema\")\n",
                "            .set_caching_options(False)\n",
                "        )\n",
                "\n",
                "        save_schema_if_not_exist_task = (\n",
                "            copy_output_to_gcs_op(\n",
                "                output_obj=gen_schema_task.outputs['X_train_schema_file'],\n",
                "                source_bucket_name=BUCKET_NAME,\n",
                "                destination_bucket_name=BUCKET_NAME,\n",
                "                destination_blob_name=f\"{BUCKET_DIR}/X_train_schema_file\",\n",
                "                overwrite_if_exists=False\n",
                "            )\n",
                "            .set_display_name(\"Save Schema For Future Run Validation if not Exists\")\n",
                "            .set_caching_options(False)\n",
                "        )\n",
                "\n",
                "        gcs_import_schema_task = (\n",
                "            kfp.dsl.importer(\n",
                "                artifact_uri=f\"gs://{BUCKET_NAME}/{BUCKET_DIR}/X_train_schema_file\",\n",
                "                artifact_class=Artifact,\n",
                "                reimport=False,\n",
                "        )\n",
                "            .set_display_name(\"Import Base Schema\")\n",
                "            .after(save_schema_if_not_exist_task)\n",
                "            .set_caching_options(False)\n",
                "        )\n",
                "\n",
                "        validate_schema_task = (\n",
                "            validate_tfdv_schema_op(\n",
                "                x_train_schema_base_file=gcs_import_schema_task.output,\n",
                "                x_train_file=bq_download_train_test_task.outputs['raw_train_ds_file']\n",
                "            )\n",
                "            .set_cpu_limit(\"2\")\n",
                "            .set_memory_limit(\"8G\")\n",
                "            .set_display_name(\"Validate Schema, Drift and Skewness\")\n",
                "            .set_caching_options(False)\n",
                "        )\n",
                "\n",
                "        with dsl.Condition(\n",
                "            validate_schema_task.outputs[\"is_abnormal\"] == \"true\",\n",
                "            name=\"Input Is Abnormal\",\n",
                "        ):\n",
                "            text_stakeholders = (f\"<@{slack_uid}>\" for slack_uid in PIPELINE_STAKEHOLDERS_SLACK_UIDS.values())\n",
                "            text_stakeholders = ' '.join(text_stakeholders)\n",
                "            text = (\n",
                "                f\"*Run name:* <{url}|{run_name}>\",\n",
                "                f\"*Run ID:* {run_id}\",\n",
                "                f\"*Stakeholders:* {text_stakeholders}. Where are you now?\"\n",
                "            )\n",
                "            text = '\\n'.join(text)\n",
                "            message = [\n",
                "                {\n",
                "                    \"type\": \"header\",\n",
                "                    \"text\": {\n",
                "                        \"type\": \"plain_text\",\n",
                "                        \"text\": \":exclamation: Kubeflow pipeline has abnormal input!\",\n",
                "                        \"emoji\": True\n",
                "                    }\n",
                "                },\n",
                "                {\n",
                "                    \"type\": \"divider\"\n",
                "                },\n",
                "                {\n",
                "                    \"type\": \"section\",\n",
                "                    \"text\": {\n",
                "                        \"type\": \"mrkdwn\",\n",
                "                        \"text\": text\n",
                "                    }\n",
                "                }\n",
                "            ]\n",
                "            slack_noti_task = slack_noti_op(\n",
                "                webhook_url=os.environ.get('SLACK_INCOMING_WEBHOOK'),\n",
                "                message=message\n",
                "            ).set_display_name(\"Input Abnormal Slack Noti\")\n",
                "\n",
                "        preprocess_train_task = (\n",
                "            preprocess_op(\n",
                "                bq_download_train_test_task.outputs['raw_train_ds_file'],\n",
                "                country_code=country_code\n",
                "            )\n",
                "            .set_cpu_limit(\"4\")\n",
                "            .set_memory_limit(\"16G\")\n",
                "            .add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
                "            .set_gpu_limit(\"1\")\n",
                "            .set_display_name(f\"Preprocess Train Data for {country_code}\")\n",
                "            .set_caching_options(False)\n",
                "        )\n",
                "\n",
                "        preprocess_test_task = (\n",
                "            preprocess_op(\n",
                "                bq_download_train_test_task.outputs['raw_test_ds_file'],\n",
                "                country_code=country_code\n",
                "            )\n",
                "            .set_cpu_limit(\"4\")\n",
                "            .set_memory_limit(\"16G\")\n",
                "            .add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
                "            .set_gpu_limit(\"1\")\n",
                "            .set_display_name(f\"Preprocess Test Data for {country_code}\")\n",
                "            .set_caching_options(False)\n",
                "        )\n",
                "\n",
                "        gcs_import_checkpoint_task = (\n",
                "            kfp.dsl.importer(\n",
                "                artifact_uri=f\"gs://{BUCKET_NAME}/{checkpoint_blob_name}\",\n",
                "                artifact_class=Artifact,\n",
                "                reimport=False,\n",
                "        )\n",
                "            .set_display_name(\"Import Checkpoint\")\n",
                "            .set_caching_options(False)\n",
                "        )\n",
                "\n",
                "        model_task = (\n",
                "            model_op(\n",
                "                train_prep_ds_file=preprocess_train_task.outputs['prep_ds_file'],\n",
                "                test_prep_ds_file=preprocess_test_task.outputs['prep_ds_file'],\n",
                "                merchant_vocab_file=preprocess_train_task.outputs['merchant_vocab_file'],\n",
                "                search_vocab_file=preprocess_train_task.outputs['search_vocab_file'],\n",
                "                checkpoint_dir=gcs_import_checkpoint_task.output,\n",
                "                resume_training=detect_resume_training_task.outputs['resume_training'],\n",
                "                country_code=country_code\n",
                "            )\n",
                "            .set_cpu_limit(\"4\")\n",
                "            .set_memory_limit(\"16G\")\n",
                "            .add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
                "            .set_gpu_limit(\"1\")\n",
                "            .set_display_name(f\"Model {country_code}\")\n",
                "            .set_caching_options(False)\n",
                "        )\n",
                "\n",
                "        evaluate_task = (\n",
                "            evaluate_op(\n",
                "                model_dir=model_task.outputs['model_output_dir'],\n",
                "                test_prep_ds_file=preprocess_test_task.outputs['prep_ds_file'],\n",
                "                merchant_vocab_file=preprocess_train_task.outputs['merchant_vocab_file'],\n",
                "                country_code=country_code,\n",
                "                k=10,\n",
                "                deployment_threshold=deployment_threshold,\n",
                "            )\n",
                "            .set_cpu_limit(\"4\")\n",
                "            .set_memory_limit(\"16G\")\n",
                "            .add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
                "            .set_gpu_limit(\"1\")\n",
                "            .set_display_name(f\"Evaluate {country_code}\")\n",
                "            .set_caching_options(False)\n",
                "        )\n",
                "\n",
                "        with dsl.Condition(\n",
                "            evaluate_task.outputs[\"dep_decision\"] == \"false\",\n",
                "            name=\"Evaluation Reports Degraded Accuracy! Deployment Cancelled!\",\n",
                "        ):\n",
                "            text_stakeholders = (f\"<@{slack_uid}>\" for slack_uid in PIPELINE_STAKEHOLDERS_SLACK_UIDS.values())\n",
                "            text_stakeholders = ' '.join(text_stakeholders)\n",
                "            text = (\n",
                "                f\"*Run name:* <{url}|{run_name}>\",\n",
                "                f\"*Run ID:* {run_id}\",\n",
                "                f\"*Stakeholders:* {text_stakeholders}. Where are you now?\"\n",
                "            )\n",
                "            text = '\\n'.join(text)\n",
                "            message = [\n",
                "                {\n",
                "                    \"type\": \"header\",\n",
                "                    \"text\": {\n",
                "                        \"type\": \"plain_text\",\n",
                "                        \"text\": \":exclamation: Evaluation reports degraded accuracy! Deployment cancelled!\",\n",
                "                        \"emoji\": True\n",
                "                    }\n",
                "                },\n",
                "                {\n",
                "                    \"type\": \"divider\"\n",
                "                },\n",
                "                {\n",
                "                    \"type\": \"section\",\n",
                "                    \"text\": {\n",
                "                        \"type\": \"mrkdwn\",\n",
                "                        \"text\": text\n",
                "                    }\n",
                "                }\n",
                "            ]\n",
                "            slack_noti_task = slack_noti_op(\n",
                "                webhook_url=os.environ.get('SLACK_INCOMING_WEBHOOK'),\n",
                "                message=message\n",
                "            ).set_display_name(\"Accuracy Degraded Slack Noti\")\n",
                "        \n",
                "        with dsl.Condition(\n",
                "            evaluate_task.outputs[\"dep_decision\"] == \"true\",\n",
                "            name=\"Deploy Decision\",\n",
                "        ):\n",
                "            preprocess_full_task = (\n",
                "                preprocess_op(\n",
                "                    bq_download_train_test_task.outputs['raw_full_ds_file'],\n",
                "                    country_code=country_code\n",
                "                )\n",
                "                .set_cpu_limit(\"4\")\n",
                "                .set_memory_limit(\"16G\")\n",
                "                .add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
                "                .set_gpu_limit(\"1\")\n",
                "                .set_display_name(f\"Preprocess Full Training Data for {country_code}\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "            \n",
                "            model_full_data_task = (\n",
                "                model_op(\n",
                "                    train_prep_ds_file=preprocess_full_task.outputs['prep_ds_file'],\n",
                "                    test_prep_ds_file=preprocess_test_task.outputs['prep_ds_file'],\n",
                "                    # Get the vocabs from preprocess_train_task not preprocess_full_task so that we can resume training\n",
                "                    merchant_vocab_file=preprocess_train_task.outputs['merchant_vocab_file'],\n",
                "                    search_vocab_file=preprocess_train_task.outputs['search_vocab_file'],\n",
                "                    checkpoint_dir=gcs_import_checkpoint_task.output,\n",
                "                    resume_training=True,\n",
                "                    country_code=country_code,\n",
                "                    cfg_overrides=['tfrs_gru.epochs=5']  # Limit 5 epochs to prevent overfit\n",
                "                )\n",
                "                .set_cpu_limit(\"4\")\n",
                "                .set_memory_limit(\"16G\")\n",
                "                .add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
                "                .set_gpu_limit(\"1\")\n",
                "                .set_display_name(f\"Model {country_code}\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            persist_model_task = (\n",
                "                copy_output_to_gcs_op(\n",
                "                    output_obj=model_full_data_task.outputs['model_output_dir'],\n",
                "                    source_bucket_name=BUCKET_NAME,\n",
                "                    destination_bucket_name=BUCKET_NAME,\n",
                "                    destination_blob_name=model_blob_name,\n",
                "                    overwrite_if_exists=True\n",
                "                )\n",
                "                .set_display_name(\"Save Model Artifacts to GCS\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            deploy_model_task = (\n",
                "                deploy_model_to_gcp_endpoint_op(\n",
                "                    model_bucket_name=BUCKET_NAME,\n",
                "                    model_blob_name=model_blob_name,\n",
                "                    model_name='seq_rec',\n",
                "                    model_version=VERSION,\n",
                "                    country_code=country_code,\n",
                "                    endpoint_id=\"\",\n",
                "                    traffic_split={\"0\": 100},\n",
                "                    undeploy_zero_traffic_models=True\n",
                "                )\n",
                "                .set_display_name(\"Deploy Model to GCP Endpoint\")\n",
                "                .after(persist_model_task)\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "            \n",
                "            persist_merchant_vocab_task = (\n",
                "                copy_output_to_gcs_op(\n",
                "                    output_obj=preprocess_train_task.outputs['merchant_vocab_file'],\n",
                "                    source_bucket_name=BUCKET_NAME,\n",
                "                    destination_bucket_name=BUCKET_NAME,\n",
                "                    destination_blob_name=os.path.join(model_blob_name, \"merchant_vocab\"),\n",
                "                    overwrite_if_exists=True\n",
                "                )\n",
                "                .set_display_name(\"Save Merchant Vocab to GCS\")\n",
                "                .after(deploy_model_task)\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "            \n",
                "            persist_search_vocab_task = (\n",
                "                copy_output_to_gcs_op(\n",
                "                    output_obj=preprocess_train_task.outputs['search_vocab_file'],\n",
                "                    source_bucket_name=BUCKET_NAME,\n",
                "                    destination_bucket_name=BUCKET_NAME,\n",
                "                    destination_blob_name=os.path.join(model_blob_name, \"search_vocab\"),\n",
                "                    overwrite_if_exists=True\n",
                "                )\n",
                "                .set_display_name(\"Save Search Vocab to GCS\")\n",
                "                .after(deploy_model_task)\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            update_user_recent_txn_in_recommend_api_task = (\n",
                "                update_user_recent_txn_in_recommend_api_op(\n",
                "                    api_key=os.environ.get(f'RECOMMEND_{ENV.upper()}_API_KEY'),\n",
                "                    env=ENV\n",
                "                )\n",
                "                .after(deploy_model_task)\n",
                "                .set_display_name(f\"Update User Recent Txn in {ENV} recommend Endpoint\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "            \n",
                "            record_last_checkpoint_date_task = (\n",
                "                record_last_checkpoint_date_op(\n",
                "                    checkpoint_bucket=BUCKET_NAME,\n",
                "                    last_checkpoint_date_blob_name=last_checkpoint_date_blob_name\n",
                "                )\n",
                "                .set_display_name(\"Record Last Checkpoint Date\")\n",
                "                .after(update_user_recent_txn_in_recommend_api_task)\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "        record_job_status_task.after(deploy_model_task)\n",
                "        record_job_status_task.after(persist_merchant_vocab_task)\n",
                "        record_job_status_task.after(persist_search_vocab_task)\n",
                "        record_job_status_task.after(evaluate_task)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "642b5578",
            "metadata": {},
            "source": [
                "### Compile the pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2b0eb485",
            "metadata": {},
            "outputs": [],
            "source": [
                "TRAINING_MODE = 'full_retraining'\n",
                "PIPELINE_ROOT_FULL = os.path.join(PIPELINE_ROOT_PREFIX, COUNTRY_CODE, TRAINING_MODE)\n",
                "COMPILED_PIPELINE_FILENAME = f\"recsys_{PIPELINE_NAME}_{COUNTRY_CODE}_{TRAINING_MODE}_pipeline.json\"\n",
                "DISPLAY_NAME = f\"recsys_{PIPELINE_NAME}_{COUNTRY_CODE}_{VERSION}_{TRAINING_MODE}_pipeline\"\n",
                "from kfp.v2 import compiler  # noqa: F811\n",
                "\n",
                "compiler.Compiler().compile(\n",
                "    pipeline_func=pipeline_full_retraining, package_path=COMPILED_PIPELINE_FILENAME\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e1956f94",
            "metadata": {},
            "source": [
                "### Run the pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5b96132d",
            "metadata": {},
            "outputs": [],
            "source": [
                "job = aip.PipelineJob(\n",
                "    display_name=DISPLAY_NAME,\n",
                "    template_path=COMPILED_PIPELINE_FILENAME,\n",
                "    pipeline_root=PIPELINE_ROOT_FULL,\n",
                "    parameter_values={\n",
                "        \"deployment_threshold\": {\n",
                "            \"hit_rate\": 0.1\n",
                "        },\n",
                "        \"resume_training\": False\n",
                "    },\n",
                "    location=REGION\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "08590891",
            "metadata": {},
            "outputs": [],
            "source": [
                "job.submit(service_account=SERVICE_ACCOUNT_EMAIL)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8b89d9ac-342e-4bab-8f50-1266100d1688",
            "metadata": {},
            "source": [
                "### Schedule"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "38afdd7a-2b03-4235-9a86-2b23ecd64625",
            "metadata": {},
            "source": [
                "#### Upload pipeline job to GCS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "867f9edc-58b9-4935-9bc0-3a6deea8a3f7",
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ['TRAINING_MODE'] = TRAINING_MODE\n",
                "os.environ['COUNTRY_CODE'] = COUNTRY_CODE\n",
                "os.environ['COMPILED_PIPELINE_FILENAME'] = COMPILED_PIPELINE_FILENAME\n",
                "os.environ['PIPELINE_ROOT'] = PIPELINE_ROOT_FULL\n",
                "os.environ['SERVICE_ACCOUNT_EMAIL'] = SERVICE_ACCOUNT_EMAIL\n",
                "os.environ['REGION'] = REGION"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "771b7f07-94af-44f9-94c3-3e8efd79cabe",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "gsutil cp ${COMPILED_PIPELINE_FILENAME} ${PIPELINE_ROOT}/${COMPILED_PIPELINE_FILENAME}\n",
                "\n",
                "echo ${PIPELINE_ROOT}/${COMPILED_PIPELINE_FILENAME}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "85a3f2d4-bf62-4672-b20c-d0550152a50b",
            "metadata": {},
            "source": [
                "#### Create Cloud Functions to submit job to Vertex AI Pipelines"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "24ab92ca-2f20-4b64-8565-36911c0e7980",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\n",
                "    PROJECT_ID,\n",
                "    REGION,\n",
                "    PIPELINE_ROOT_FULL,\n",
                "    SERVICE_ACCOUNT_EMAIL,\n",
                "    DISPLAY_NAME,\n",
                "    sep='\\n'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e54f2511-2a4b-463d-9c0e-addaabe207bf",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writetemplate main.py\n",
                "\n",
                "import json\n",
                "from google.cloud import aiplatform\n",
                "import functions_framework\n",
                "\n",
                "PROJECT_ID = \"{PROJECT_ID}\"\n",
                "REGION = \"{REGION}\"\n",
                "PIPELINE_ROOT = \"{PIPELINE_ROOT_FULL}\"\n",
                "SERVICE_ACCOUNT_EMAIL = \"{SERVICE_ACCOUNT_EMAIL}\"\n",
                "DISPLAY_NAME = \"{DISPLAY_NAME}\"\n",
                "\n",
                "@functions_framework.http\n",
                "def recsys_seq_rec_{COUNTRY_CODE}_{TRAINING_MODE}_vertex_ai_pipeline_job(request):\n",
                "    \"\"\"Processes the incoming HTTP request.\n",
                "\n",
                "    Args:\n",
                "     request (flask.Request): HTTP request object.\n",
                "\n",
                "    Returns:\n",
                "     The response text or any set of values that can be turned into a Response\n",
                "     object using `make_response\n",
                "     <http://flask.pocoo.org/docs/1.0/api/#flask.Flask.make_response>`.\n",
                "    \"\"\"\n",
                "    import subprocess\n",
                "    import os\n",
                "    \n",
                "    # decode http request payload and translate into JSON object\n",
                "    request_str = request.data.decode('utf-8')\n",
                "    request_json = json.loads(request_str)\n",
                "\n",
                "    pipeline_spec_uri = request_json['pipeline_spec_uri']\n",
                "    parameter_values = request_json['parameter_values']\n",
                "\n",
                "    aiplatform.init(\n",
                "        project=PROJECT_ID,\n",
                "        location=REGION\n",
                "    )\n",
                "\n",
                "    job = aiplatform.PipelineJob(\n",
                "        display_name=DISPLAY_NAME,\n",
                "        template_path=pipeline_spec_uri,\n",
                "        pipeline_root=PIPELINE_ROOT,\n",
                "        enable_caching=False,\n",
                "        parameter_values=parameter_values,\n",
                "    )\n",
                "\n",
                "    job.submit(service_account=SERVICE_ACCOUNT_EMAIL)\n",
                "\n",
                "    return \"Job submitted\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4fa9aad3-bb1c-4900-b620-d6a6e0f54f05",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writefile requirements.txt\n",
                "\n",
                "google-api-python-client>=1.7.8,<2\n",
                "google-cloud-aiplatform\n",
                "google-cloud-storage\n",
                "PyYAML"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e1236c77-92f5-42ff-bf8b-e31a9cc5aa6b",
            "metadata": {},
            "outputs": [],
            "source": [
                "FUNCTION_DIR = '../functions/deploy_pipeline'\n",
                "\n",
                "os.environ['FUNCTION_DIR'] = FUNCTION_DIR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b3aaee18-72a7-4fb3-bf37-5231aa586370",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "mkdir -p $FUNCTION_DIR\n",
                "mv main.py $FUNCTION_DIR\n",
                "mv requirements.txt $FUNCTION_DIR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "814cf822-887b-4be8-848b-3c125dbbf387",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "cd $FUNCTION_DIR\n",
                "\n",
                "gcloud functions deploy recsys_seq_rec_${COUNTRY_CODE}_${TRAINING_MODE}_vertex_ai_pipeline_job \\\n",
                "    --runtime python39 --trigger-http \\\n",
                "    --service-account $SERVICE_ACCOUNT_EMAIL"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6824e793-87ba-46da-9235-a35efdce2d8a",
            "metadata": {},
            "source": [
                "#### Cloud Scheduler"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "915a6d46-e41f-499f-aa83-6854f78cdc5c",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "read -r -d '' message_body << EOM\n",
                "{\n",
                "    \"pipeline_spec_uri\": \"${PIPELINE_ROOT}/${COMPILED_PIPELINE_FILENAME}\",\n",
                "    \"parameter_values\": {\n",
                "        \"deployment_threshold\": {\n",
                "            \"hit_rate\": 0.10\n",
                "        },\n",
                "        \"resume_training\": false\n",
                "    }\n",
                "}\n",
                "EOM\n",
                "\n",
                "# Run at 1:00AM every Monday\n",
                "cron=\"0 1 * * 1\"\n",
                "\n",
                "# https://cloud.google.com/scheduler/docs/http-target-auth#using-gcloud\n",
                "\n",
                "gcloud scheduler jobs create http recsys_seq_rec_${COUNTRY_CODE}_${TRAINING_MODE}_vertex_ai_pipeline_job \\\n",
                "    --schedule=\"$cron\" \\\n",
                "    --uri=\"https://us-central1-seq-rec-gcp-project-id.cloudfunctions.net/recsys_seq_rec_${COUNTRY_CODE}_${TRAINING_MODE}_vertex_ai_pipeline_job\" \\\n",
                "    --http-method=POST \\\n",
                "    --message-body=\"$message_body\" \\\n",
                "    --time-zone=\"Asia/Singapore\" \\\n",
                "    --oidc-service-account-email=$SERVICE_ACCOUNT_EMAIL \\\n",
                "    --location=$REGION"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "95043443-8540-4eb4-82e6-4de1bcaf7579",
            "metadata": {},
            "source": [
                "## Incremental Retraining"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "abf13310-4562-496d-a8d7-0b45dacc41e2",
            "metadata": {},
            "outputs": [],
            "source": [
                "@dsl.pipeline(\n",
                "    # Default pipeline root. You can override it when submitting the pipeline.\n",
                "    pipeline_root=PIPELINE_ROOT_PREFIX,\n",
                "    # A name for the pipeline. Use to determine the pipeline Context.\n",
                "    name=f\"{PIPELINE_NAME}{COUNTRY_CODE.lower()}incrementalretraining\",\n",
                ")\n",
                "def pipeline_incremental_retraining(\n",
                "        deployment_threshold: dict,\n",
                "        resume_training: bool = True,\n",
                "    ):\n",
                "    \"\"\" Define the pipeline components for incremental retraining\n",
                "\n",
                "    Args:\n",
                "        deployment_threshold (dict): dictionary of metrics and thresholds to deploy\n",
                "        resume_training (bool): whether to pick up the model state from previous train and continue training\n",
                "    \"\"\"\n",
                "    run_id = dsl.PIPELINE_JOB_ID_PLACEHOLDER\n",
                "    run_name = dsl.PIPELINE_JOB_NAME_PLACEHOLDER\n",
                "    resource_name = dsl.PIPELINE_JOB_RESOURCE_NAME_PLACEHOLDER\n",
                "    run_status = '{{$.pipeline_job_status}} - {{$.pipeline_status}} - {{$.pipeline_worlflow_status}}'\n",
                "    url = f\"https://console.cloud.google.com/vertex-ai/locations/{REGION}/pipelines/runs/{run_name}?project={PROJECT_ID}\"\n",
                "    \n",
                "    country_code = COUNTRY_CODE\n",
                "    model_blob_name = f\"{BUCKET_DIR}/{VERSION}/{country_code}/model/\"\n",
                "    checkpoint_blob_name = f\"{BUCKET_DIR}/{VERSION}/{country_code}/checkpoint/\"\n",
                "    pipeline_root_full = os.path.join(PIPELINE_ROOT_PREFIX, country_code, \"incremental_retraining\")\n",
                "    last_checkpoint_date_blob_name = os.path.join(checkpoint_blob_name, \"last_checkpoint_date.txt\")\n",
                "    \n",
                "    exit_text = (\n",
                "        f\"*Run name:* <{url}|{run_name}>\",\n",
                "        f\"*Run ID:* {run_id}\",\n",
                "    )\n",
                "    exit_text = '\\n'.join(exit_text)\n",
                "    message = [\n",
                "        {\n",
                "            \"type\": \"header\",\n",
                "            \"text\": {\n",
                "                \"type\": \"plain_text\",\n",
                "                \"text\": \"Kubeflow pipeline has completed!\",\n",
                "                \"emoji\": True\n",
                "            }\n",
                "        },\n",
                "        {\n",
                "            \"type\": \"divider\"\n",
                "        },\n",
                "        {\n",
                "            \"type\": \"section\",\n",
                "            \"text\": {\n",
                "                \"type\": \"mrkdwn\",\n",
                "                \"text\": exit_text\n",
                "            }\n",
                "        }\n",
                "    ]\n",
                "    \n",
                "    slack_noti_exit_task = slack_noti_exit_op(\n",
                "        webhook_url=os.environ.get('SLACK_INCOMING_WEBHOOK'),  # Please declare the SLACK_INCOMING_WEBHOOK as a var in .env file\n",
                "        message=message,\n",
                "        run_name=run_name,\n",
                "        job_status_file_name=JOB_STATUS_FILE_NAME,\n",
                "        bucket_url=BUCKET_URL,\n",
                "        folder=BUCKET_DIR,\n",
                "        pipeline_stakeholders_slack_uids=PIPELINE_STAKEHOLDERS_SLACK_UIDS\n",
                "    ).set_display_name(\"Exit Handler Slack Noti\")\n",
                "    \n",
                "    # Could not get the job status via pipeline utils so create a small hack here\n",
                "    # The below task should be triggered after the final tasks has completed\n",
                "    record_job_status_task = (\n",
                "        record_job_status_op(\n",
                "            run_name=run_name,\n",
                "            job_status_file_name=JOB_STATUS_FILE_NAME,\n",
                "            bucket_name=BUCKET_NAME,\n",
                "            folder=BUCKET_DIR,\n",
                "        )\n",
                "        .set_display_name(\"Record Job Status\")\n",
                "    )\n",
                "    # Since the input of this task does not change as we overwrite the same file, we need to disable caching for this task.\n",
                "    # record_job_status_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
                "    record_job_status_task.set_caching_options(False)\n",
                "\n",
                "    with dsl.ExitHandler(slack_noti_exit_task):\n",
                "        detect_resume_training_task = (\n",
                "            detect_resume_training_op(\n",
                "                resume_training=resume_training,\n",
                "                checkpoint_bucket=BUCKET_NAME,\n",
                "                last_checkpoint_date_blob_name=last_checkpoint_date_blob_name\n",
                "            )\n",
                "            .set_display_name(\"Detect and Prepare to Resume Training\")\n",
                "            .set_caching_options(False)\n",
                "        )\n",
                "\n",
                "        with dsl.Condition(\n",
                "            detect_resume_training_task.outputs[\"start_training\"] == \"true\",\n",
                "            name=\"Start Training\",\n",
                "        ):\n",
                "            bq_download_train_test_task = (\n",
                "                tf_download_bq_table_op(\n",
                "                    resume_training=detect_resume_training_task.outputs['resume_training'],\n",
                "                    last_checkpoint_date=detect_resume_training_task.outputs['last_checkpoint_date'],\n",
                "                    country_code=country_code\n",
                "                )\n",
                "                .set_cpu_limit(\"4\")\n",
                "                .set_memory_limit(\"16G\")\n",
                "                .add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
                "                .set_gpu_limit(\"1\")\n",
                "                .set_display_name(\"Download Train and Test Data for Evaluation\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            gen_schema_task = (\n",
                "                generate_tfdv_schema_op(\n",
                "                    x_train_file=bq_download_train_test_task.outputs['raw_train_ds_file']\n",
                "                )\n",
                "                .set_display_name(\"Generate Schema\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            save_schema_if_not_exist_task = (\n",
                "                copy_output_to_gcs_op(\n",
                "                    output_obj=gen_schema_task.outputs['X_train_schema_file'],\n",
                "                    source_bucket_name=BUCKET_NAME,\n",
                "                    destination_bucket_name=BUCKET_NAME,\n",
                "                    destination_blob_name=f\"{BUCKET_DIR}/X_train_schema_file\",\n",
                "                    overwrite_if_exists=False\n",
                "                )\n",
                "                .set_display_name(\"Save Schema For Future Run Validation if not Exists\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            gcs_import_schema_task = (\n",
                "                kfp.dsl.importer(\n",
                "                    artifact_uri=f\"gs://{BUCKET_NAME}/{BUCKET_DIR}/X_train_schema_file\",\n",
                "                    artifact_class=Artifact,\n",
                "                    reimport=False,\n",
                "            )\n",
                "                .set_display_name(\"Import Base Schema\")\n",
                "                .after(save_schema_if_not_exist_task)\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            validate_schema_task = (\n",
                "                validate_tfdv_schema_op(\n",
                "                    x_train_schema_base_file=gcs_import_schema_task.output,\n",
                "                    x_train_file=bq_download_train_test_task.outputs['raw_train_ds_file']\n",
                "                )\n",
                "                .set_cpu_limit(\"2\")\n",
                "                .set_memory_limit(\"8G\")\n",
                "                .set_display_name(\"Validate Schema, Drift and Skewness\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            with dsl.Condition(\n",
                "                validate_schema_task.outputs[\"is_abnormal\"] == \"true\",\n",
                "                name=\"Input Is Abnormal\",\n",
                "            ):\n",
                "                text_stakeholders = (f\"<@{slack_uid}>\" for slack_uid in PIPELINE_STAKEHOLDERS_SLACK_UIDS.values())\n",
                "                text_stakeholders = ' '.join(text_stakeholders)\n",
                "                text = (\n",
                "                    f\"*Run name:* <{url}|{run_name}>\",\n",
                "                    f\"*Run ID:* {run_id}\",\n",
                "                    f\"*Stakeholders:* {text_stakeholders}. Where are you now?\"\n",
                "                )\n",
                "                text = '\\n'.join(text)\n",
                "                message = [\n",
                "                    {\n",
                "                        \"type\": \"header\",\n",
                "                        \"text\": {\n",
                "                            \"type\": \"plain_text\",\n",
                "                            \"text\": \":exclamation: Kubeflow pipeline has abnormal input!\",\n",
                "                            \"emoji\": True\n",
                "                        }\n",
                "                    },\n",
                "                    {\n",
                "                        \"type\": \"divider\"\n",
                "                    },\n",
                "                    {\n",
                "                        \"type\": \"section\",\n",
                "                        \"text\": {\n",
                "                            \"type\": \"mrkdwn\",\n",
                "                            \"text\": text\n",
                "                        }\n",
                "                    }\n",
                "                ]\n",
                "                slack_noti_task = slack_noti_op(\n",
                "                    webhook_url=os.environ.get('SLACK_INCOMING_WEBHOOK'),\n",
                "                    message=message\n",
                "                ).set_display_name(\"Input Abnormal Slack Noti\")\n",
                "\n",
                "            preprocess_train_task = (\n",
                "                preprocess_op(\n",
                "                    bq_download_train_test_task.outputs['raw_train_ds_file'],\n",
                "                    country_code=country_code\n",
                "                )\n",
                "                .set_cpu_limit(\"4\")\n",
                "                .set_memory_limit(\"16G\")\n",
                "                .add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
                "                .set_gpu_limit(\"1\")\n",
                "                .set_display_name(f\"Preprocess Train Data for {country_code}\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            preprocess_test_task = (\n",
                "                preprocess_op(\n",
                "                    bq_download_train_test_task.outputs['raw_test_ds_file'],\n",
                "                    country_code=country_code\n",
                "                )\n",
                "                .set_cpu_limit(\"4\")\n",
                "                .set_memory_limit(\"16G\")\n",
                "                .add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
                "                .set_gpu_limit(\"1\")\n",
                "                .set_display_name(f\"Preprocess Test Data for {country_code}\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            gcs_import_checkpoint_task = (\n",
                "                kfp.dsl.importer(\n",
                "                    artifact_uri=f\"gs://{BUCKET_NAME}/{checkpoint_blob_name}\",\n",
                "                    artifact_class=Artifact,\n",
                "                    reimport=False,\n",
                "            )\n",
                "                .set_display_name(\"Import Checkpoint\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            gcs_import_merchant_vocab_task = (\n",
                "                kfp.dsl.importer(\n",
                "                    artifact_uri=os.path.join(\"gs://\", BUCKET_NAME, model_blob_name, \"merchant_vocab\"),\n",
                "                    artifact_class=Dataset,\n",
                "                    reimport=False,\n",
                "            )\n",
                "                .set_display_name(\"Import Checkpoint Merchant Vocab\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            gcs_import_search_vocab_task = (\n",
                "                kfp.dsl.importer(\n",
                "                    artifact_uri=os.path.join(\"gs://\", BUCKET_NAME, model_blob_name, \"search_vocab\"),\n",
                "                    artifact_class=Dataset,\n",
                "                    reimport=False,\n",
                "            )\n",
                "                .set_display_name(\"Import Checkpoint Search Vocab\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            merchant_vocab_artifact = gcs_import_merchant_vocab_task.output\n",
                "            search_vocab_artifact = gcs_import_search_vocab_task.output\n",
                "\n",
                "            model_task = (\n",
                "                model_op(\n",
                "                    train_prep_ds_file=preprocess_train_task.outputs['prep_ds_file'],\n",
                "                    test_prep_ds_file=preprocess_test_task.outputs['prep_ds_file'],\n",
                "                    merchant_vocab_file=merchant_vocab_artifact,\n",
                "                    search_vocab_file=search_vocab_artifact,\n",
                "                    checkpoint_dir=gcs_import_checkpoint_task.output,\n",
                "                    resume_training=detect_resume_training_task.outputs['resume_training'],\n",
                "                    country_code=country_code,\n",
                "                    cfg_overrides=['tfrs_gru.epochs=1']  # Limit 1 epochs to prevent overfit\n",
                "                )\n",
                "                .set_cpu_limit(\"4\")\n",
                "                .set_memory_limit(\"16G\")\n",
                "                .add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
                "                .set_gpu_limit(\"1\")\n",
                "                .set_display_name(f\"Model {country_code}\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            evaluate_task = (\n",
                "                evaluate_op(\n",
                "                    model_dir=model_task.outputs['model_output_dir'],\n",
                "                    test_prep_ds_file=preprocess_test_task.outputs['prep_ds_file'],\n",
                "                    merchant_vocab_file=merchant_vocab_artifact,\n",
                "                    country_code=country_code,\n",
                "                    k=10,\n",
                "                    deployment_threshold=deployment_threshold,\n",
                "                )\n",
                "                .set_cpu_limit(\"4\")\n",
                "                .set_memory_limit(\"16G\")\n",
                "                .add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
                "                .set_gpu_limit(\"1\")\n",
                "                .set_display_name(f\"Evaluate {country_code}\")\n",
                "                .set_caching_options(False)\n",
                "            )\n",
                "\n",
                "            with dsl.Condition(\n",
                "                evaluate_task.outputs[\"dep_decision\"] == \"false\",\n",
                "                name=\"Evaluation Reports Degraded Accuracy! Deployment Cancelled!\",\n",
                "            ):\n",
                "                text_stakeholders = (f\"<@{slack_uid}>\" for slack_uid in PIPELINE_STAKEHOLDERS_SLACK_UIDS.values())\n",
                "                text_stakeholders = ' '.join(text_stakeholders)\n",
                "                text = (\n",
                "                    f\"*Run name:* <{url}|{run_name}>\",\n",
                "                    f\"*Run ID:* {run_id}\",\n",
                "                    f\"*Stakeholders:* {text_stakeholders}. Where are you now?\"\n",
                "                )\n",
                "                text = '\\n'.join(text)\n",
                "                message = [\n",
                "                    {\n",
                "                        \"type\": \"header\",\n",
                "                        \"text\": {\n",
                "                            \"type\": \"plain_text\",\n",
                "                            \"text\": \":exclamation: Evaluation reports degraded accuracy! Deployment cancelled!\",\n",
                "                            \"emoji\": True\n",
                "                        }\n",
                "                    },\n",
                "                    {\n",
                "                        \"type\": \"divider\"\n",
                "                    },\n",
                "                    {\n",
                "                        \"type\": \"section\",\n",
                "                        \"text\": {\n",
                "                            \"type\": \"mrkdwn\",\n",
                "                            \"text\": text\n",
                "                        }\n",
                "                    }\n",
                "                ]\n",
                "                slack_noti_task = slack_noti_op(\n",
                "                    webhook_url=os.environ.get('SLACK_INCOMING_WEBHOOK'),\n",
                "                    message=message\n",
                "                ).set_display_name(\"Accuracy Degraded Slack Noti\")\n",
                "\n",
                "            with dsl.Condition(\n",
                "                evaluate_task.outputs[\"dep_decision\"] == \"true\",\n",
                "                name=\"Deploy Decision\",\n",
                "            ):\n",
                "                preprocess_full_task = (\n",
                "                    preprocess_op(\n",
                "                        bq_download_train_test_task.outputs['raw_full_ds_file'],\n",
                "                        country_code=country_code\n",
                "                    )\n",
                "                    .set_cpu_limit(\"4\")\n",
                "                    .set_memory_limit(\"16G\")\n",
                "                    .add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
                "                    .set_gpu_limit(\"1\")\n",
                "                    .set_display_name(f\"Preprocess Full Training Data for {country_code}\")\n",
                "                    .set_caching_options(False)\n",
                "                )\n",
                "\n",
                "                model_full_data_task = (\n",
                "                    model_op(\n",
                "                        # Use test dataset here because this is incremental training, previously model has already been trained on train data\n",
                "                        train_prep_ds_file=preprocess_test_task.outputs['prep_ds_file'],\n",
                "                        # Use preprocess_train_task here as mock, since use_val_ds = False we don't really use this dataset\n",
                "                        # However, if set it to preprocess_test_task then weird error train_prep_ds_file will be None.\n",
                "                        test_prep_ds_file=preprocess_train_task.outputs['prep_ds_file'],\n",
                "                        use_val_ds=False,\n",
                "                        merchant_vocab_file=merchant_vocab_artifact,\n",
                "                        search_vocab_file=search_vocab_artifact,\n",
                "                        checkpoint_dir=gcs_import_checkpoint_task.output,\n",
                "                        resume_training=detect_resume_training_task.outputs['resume_training'],\n",
                "                        country_code=country_code,\n",
                "                        cfg_overrides=['tfrs_gru.epochs=5']  # Limit 5 epochs to prevent overfit\n",
                "                    )\n",
                "                    .set_cpu_limit(\"4\")\n",
                "                    .set_memory_limit(\"16G\")\n",
                "                    .add_node_selector_constraint('cloud.google.com/gke-accelerator', 'NVIDIA_TESLA_T4')\n",
                "                    .set_gpu_limit(\"1\")\n",
                "                    .set_display_name(f\"Model {country_code}\")\n",
                "                    .set_caching_options(False)\n",
                "                )\n",
                "\n",
                "                persist_model_task = (\n",
                "                    copy_output_to_gcs_op(\n",
                "                        output_obj=model_full_data_task.outputs['model_output_dir'],\n",
                "                        source_bucket_name=BUCKET_NAME,\n",
                "                        destination_bucket_name=BUCKET_NAME,\n",
                "                        destination_blob_name=model_blob_name,\n",
                "                        overwrite_if_exists=True\n",
                "                    )\n",
                "                    .set_display_name(\"Save Model Artifacts to GCS\")\n",
                "                    .set_caching_options(False)\n",
                "                )\n",
                "\n",
                "                deploy_model_task = (\n",
                "                    deploy_model_to_gcp_endpoint_op(\n",
                "                        model_bucket_name=BUCKET_NAME,\n",
                "                        model_blob_name=model_blob_name,\n",
                "                        model_name='seq_rec',\n",
                "                        model_version=VERSION,\n",
                "                        country_code=country_code,\n",
                "                        endpoint_id=\"\",\n",
                "                        traffic_split={\"0\": 100},\n",
                "                        undeploy_zero_traffic_models=True\n",
                "                    )\n",
                "                    .set_display_name(\"Deploy Model to GCP Endpoint\")\n",
                "                    .after(persist_model_task)\n",
                "                    .set_caching_options(False)\n",
                "                )\n",
                "\n",
                "                record_last_checkpoint_date_task = (\n",
                "                    record_last_checkpoint_date_op(\n",
                "                        checkpoint_bucket=BUCKET_NAME,\n",
                "                        last_checkpoint_date_blob_name=last_checkpoint_date_blob_name\n",
                "                    )\n",
                "                    .set_display_name(\"Record Last Checkpoint Date\")\n",
                "                    .after(deploy_model_task)\n",
                "                    .set_caching_options(False)\n",
                "                )\n",
                "\n",
                "            record_job_status_task.after(deploy_model_task)\n",
                "            record_job_status_task.after(evaluate_task)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "33372637-f1a5-4f5b-93ed-d68a47ecad7c",
            "metadata": {},
            "source": [
                "### Compile the pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1619954d-2a02-47bb-adc0-ba7c4c3d4a3c",
            "metadata": {},
            "outputs": [],
            "source": [
                "TRAINING_MODE = 'incremental_retraining'\n",
                "PIPELINE_ROOT_FULL = os.path.join(PIPELINE_ROOT_PREFIX, COUNTRY_CODE, TRAINING_MODE)\n",
                "COMPILED_PIPELINE_FILENAME = f\"recsys_{PIPELINE_NAME}_{COUNTRY_CODE}_{TRAINING_MODE}_pipeline.json\"\n",
                "DISPLAY_NAME = f\"recsys_{PIPELINE_NAME}_{COUNTRY_CODE}_{VERSION}_{TRAINING_MODE}_pipeline\"\n",
                "from kfp.v2 import compiler  # noqa: F811\n",
                "\n",
                "compiler.Compiler().compile(\n",
                "    pipeline_func=pipeline_incremental_retraining, package_path=COMPILED_PIPELINE_FILENAME\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "feef2942-8731-4365-9c84-ce5ea12cc953",
            "metadata": {},
            "source": [
                "### Run the pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "795d332d-0d21-4f1a-97e7-d0c9203ad37d",
            "metadata": {},
            "outputs": [],
            "source": [
                "job = aip.PipelineJob(\n",
                "    display_name=DISPLAY_NAME,\n",
                "    template_path=COMPILED_PIPELINE_FILENAME,\n",
                "    pipeline_root=PIPELINE_ROOT_FULL,\n",
                "    parameter_values={\n",
                "        \"deployment_threshold\": {\n",
                "            \"hit_rate\": 0.10\n",
                "        },\n",
                "        \"resume_training\": True\n",
                "    },\n",
                "    location=REGION\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "90684a49-71f6-42f9-a1f8-3f036c44e500",
            "metadata": {},
            "outputs": [],
            "source": [
                "job.submit(service_account=SERVICE_ACCOUNT_EMAIL)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dafda10a-a3e4-44d6-afa3-84b564435756",
            "metadata": {},
            "source": [
                "### Schedule"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1bfe655b-fcf6-4de8-b3d9-825d36c2a01f",
            "metadata": {},
            "source": [
                "#### Upload pipeline job to GCS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "91b42653-37a4-4c82-8991-0590120f2618",
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ['TRAINING_MODE'] = TRAINING_MODE\n",
                "os.environ['COUNTRY_CODE'] = COUNTRY_CODE\n",
                "os.environ['COMPILED_PIPELINE_FILENAME'] = COMPILED_PIPELINE_FILENAME\n",
                "os.environ['PIPELINE_ROOT'] = PIPELINE_ROOT_FULL\n",
                "os.environ['SERVICE_ACCOUNT_EMAIL'] = SERVICE_ACCOUNT_EMAIL\n",
                "os.environ['REGION'] = REGION"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f888a250-7c47-4416-a75f-a63706d9cc9d",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "gsutil cp ${COMPILED_PIPELINE_FILENAME} ${PIPELINE_ROOT}/${COMPILED_PIPELINE_FILENAME}\n",
                "\n",
                "echo ${PIPELINE_ROOT}/${COMPILED_PIPELINE_FILENAME}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2062797e-16ef-4a0c-9f52-3e1126a07666",
            "metadata": {},
            "source": [
                "#### Create Cloud Functions to submit job to Vertex AI Pipelines"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2f696ea8-6ebf-4eb4-bbd6-f9c57df16993",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\n",
                "    PROJECT_ID,\n",
                "    REGION,\n",
                "    PIPELINE_ROOT_FULL,\n",
                "    SERVICE_ACCOUNT_EMAIL,\n",
                "    DISPLAY_NAME,\n",
                "    sep='\\n'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "40420429-1aff-4c62-a174-1e6c4bef2f18",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writetemplate main.py\n",
                "\n",
                "import json\n",
                "from google.cloud import aiplatform\n",
                "import functions_framework\n",
                "\n",
                "PROJECT_ID = \"{PROJECT_ID}\"\n",
                "REGION = \"{REGION}\"\n",
                "PIPELINE_ROOT = \"{PIPELINE_ROOT_FULL}\"\n",
                "SERVICE_ACCOUNT_EMAIL = \"{SERVICE_ACCOUNT_EMAIL}\"\n",
                "DISPLAY_NAME = \"{DISPLAY_NAME}\"\n",
                "\n",
                "@functions_framework.http\n",
                "def recsys_seq_rec_{COUNTRY_CODE}_{TRAINING_MODE}_pipeline_job(request):\n",
                "    \"\"\"Processes the incoming HTTP request.\n",
                "\n",
                "    Args:\n",
                "     request (flask.Request): HTTP request object.\n",
                "\n",
                "    Returns:\n",
                "     The response text or any set of values that can be turned into a Response\n",
                "     object using `make_response\n",
                "     <http://flask.pocoo.org/docs/1.0/api/#flask.Flask.make_response>`.\n",
                "    \"\"\"\n",
                "    import subprocess\n",
                "    import os\n",
                "    \n",
                "    # decode http request payload and translate into JSON object\n",
                "    request_str = request.data.decode('utf-8')\n",
                "    request_json = json.loads(request_str)\n",
                "\n",
                "    pipeline_spec_uri = request_json['pipeline_spec_uri']\n",
                "    parameter_values = request_json['parameter_values']\n",
                "\n",
                "    aiplatform.init(\n",
                "        project=PROJECT_ID,\n",
                "        location=REGION\n",
                "    )\n",
                "\n",
                "    job = aiplatform.PipelineJob(\n",
                "        display_name=DISPLAY_NAME,\n",
                "        template_path=pipeline_spec_uri,\n",
                "        pipeline_root=PIPELINE_ROOT,\n",
                "        enable_caching=False,\n",
                "        parameter_values=parameter_values,\n",
                "    )\n",
                "\n",
                "    job.submit(service_account=SERVICE_ACCOUNT_EMAIL)\n",
                "\n",
                "    return \"Job submitted\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f28de14f-6413-4a83-bd44-4bfbdf063345",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%writefile requirements.txt\n",
                "\n",
                "google-api-python-client>=1.7.8,<2\n",
                "google-cloud-aiplatform\n",
                "google-cloud-storage\n",
                "PyYAML"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ddd1fe58-3257-4e3a-ac99-ede4acc99177",
            "metadata": {},
            "outputs": [],
            "source": [
                "FUNCTION_DIR = '../functions/deploy_pipeline'\n",
                "\n",
                "os.environ['FUNCTION_DIR'] = FUNCTION_DIR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "592f3969-66f2-42ba-a10d-28e555696d68",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "mkdir -p $FUNCTION_DIR\n",
                "mv main.py $FUNCTION_DIR\n",
                "mv requirements.txt $FUNCTION_DIR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6486daa9-d089-4426-89cd-0f8c62332938",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "cd $FUNCTION_DIR\n",
                "\n",
                "gcloud functions deploy recsys_seq_rec_${COUNTRY_CODE}_${TRAINING_MODE}_pipeline_job \\\n",
                "    --runtime python39 --trigger-http \\\n",
                "    --service-account $SERVICE_ACCOUNT_EMAIL"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d1150c6e-5ba6-4afe-b09f-6caedfbc08b5",
            "metadata": {},
            "source": [
                "#### Cloud Scheduler"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4e4504f8-fd6f-443d-91e6-538e3b18ccae",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%bash\n",
                "\n",
                "read -r -d '' message_body << EOM\n",
                "{\n",
                "    \"pipeline_spec_uri\": \"${PIPELINE_ROOT}/${COMPILED_PIPELINE_FILENAME}\",\n",
                "    \"parameter_values\": {\n",
                "        \"deployment_threshold\": {\n",
                "            \"hit_rate\": 0.10\n",
                "        },\n",
                "        \"resume_training\": true\n",
                "    }\n",
                "}\n",
                "EOM\n",
                "\n",
                "# Run at 4:00AM every day\n",
                "# Should run after the normal run of full_training pipeline because then we can skip the incremental retraining for that same day\n",
                "cron=\"0 4 * * *\"\n",
                "\n",
                "# https://cloud.google.com/scheduler/docs/http-target-auth#using-gcloud\n",
                "\n",
                "gcloud scheduler jobs create http recsys_seq_rec_${COUNTRY_CODE}_${TRAINING_MODE}_pipeline_job \\\n",
                "    --schedule=\"$cron\" \\\n",
                "    --uri=\"https://us-central1-seq-rec-gcp-project-id.cloudfunctions.net/recsys_seq_rec_${COUNTRY_CODE}_${TRAINING_MODE}_pipeline_job\" \\\n",
                "    --http-method=POST \\\n",
                "    --message-body=\"$message_body\" \\\n",
                "    --time-zone=\"Asia/Singapore\" \\\n",
                "    --oidc-service-account-email=$SERVICE_ACCOUNT_EMAIL \\\n",
                "    --location=$REGION"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "67980754-0d53-4b2b-ac44-1eceaa29936a",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.14 ('.venv': poetry)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.14"
        },
        "vscode": {
            "interpreter": {
                "hash": "eaeefced7af4788b9b4a203895b09193a6dd449d207d1b1237ec1e0a47ffeed0"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
